{"cells":[{"cell_type":"markdown","source":["# Fabric Scanner API to capture all Fabric artifcats' grants and access information\n","\n","The steps below provide details to have a Service principal read the Fabric metadata. Currently, there is no way in Fabric to show what artifacts are shared with whom. We will capture this metadata via the Scanner APIs\n","\n","https://learn.microsoft.com/en-us/fabric/governance/metadata-scanning-overview\n","\n","Prerequisites:\n","- Follow this documentation to enable SP authentication for read-only Admin APIs\n","    https://learn.microsoft.com/en-us/fabric/admin/metadata-scanning-enable-read-only-apis\n","- \tThe APIs we are going to use are GetModifiedWorkspaces, WorkspacegetInfo, WorkspacescanStatus and WorkspacescanResult\n","- See this documentation for further guidance -\n","\n","    https://learn.microsoft.com/en-us/fabric/admin/metadata-scanning-setup\n","    \n","    https://learn.microsoft.com/en-us/fabric/governance/metadata-scanning-run\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4a6ff3ea-f654-438c-9c9a-f771997693d1"},{"cell_type":"markdown","source":["# Notebook overview\n","\n","This notebook captures Fabric artifact user grants and accesses via Scanner APIs, transforms them into a  standard format (as each artifact's metadata output is not necessarily consistent). We are storing the final output in a table in the lakehouse and create Power BI reports to provide the relevant information\n","\n","Future Enhancements planned:\n","- Divide list of workspace ids in chunks of 100 workspaces and execute each chunk\n","- Implement incremental scan"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2ef315a4-4ca8-419b-ac49-f640763f3ce9"},{"cell_type":"markdown","source":["<mark>**Please ensure notebook setup is done in the next 2 cells before running the notebook**</mark>"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6ec5a646-ff12-4fae-bb26-c331f18f6425"},{"cell_type":"code","source":["# Enter details of workspace, lakehouse where the final table called 'WorkspaceAccessMetadata' should reside in\n","myLakehouse = 'lakehouse01'\n","myWorkspace = 'WS_SagarFabric01'\n","myTablePath = \"abfss://\"+myWorkspace+\"@onelake.dfs.fabric.microsoft.com/\"+myLakehouse+\".Lakehouse/Tables/WorkspaceAccessMetadata\"\n","print(myTablePath)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"6e945aa3-363c-4cdf-9289-2cbdc3e9199b","statement_id":40,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-04T22:54:35.6358121Z","session_start_time":null,"execution_start_time":"2024-03-04T22:54:35.9784404Z","execution_finish_time":"2024-03-04T22:54:36.2123008Z","parent_msg_id":"5ff5c499-d33d-41d3-a8f9-ab9478012b10"},"text/plain":"StatementMeta(, 6e945aa3-363c-4cdf-9289-2cbdc3e9199b, 40, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["abfss://WS_SagarFabric01@onelake.dfs.fabric.microsoft.com/lakehouse01.Lakehouse/Tables/WorkspaceAccessMetadata\n"]}],"execution_count":38,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0cb17e88-e851-4501-8b2c-f1f4d7d16f71"},{"cell_type":"code","source":["# Read secrets from Azure Key Vault\n","# Ensure the following items are defined in the keyvault before running the notebook\n","key_vault = \"https://pvlab-9a758f-keyvault.vault.azure.net/\"\n","client_secret = mssparkutils.credentials.getSecret(key_vault , \"secret-app-Fabric-Power-API\")\n","tenant = mssparkutils.credentials.getSecret(key_vault , \"FabricTenantId\")\n","client = mssparkutils.credentials.getSecret(key_vault , \"app-Fabric-Power-API-ClientID\")\n","#print(tenant)\n","#print(client)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"6e945aa3-363c-4cdf-9289-2cbdc3e9199b","statement_id":41,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-04T22:54:35.7532975Z","session_start_time":null,"execution_start_time":"2024-03-04T22:54:37.269086Z","execution_finish_time":"2024-03-04T22:54:38.7663642Z","parent_msg_id":"bf570ede-847d-49af-a018-49627e87c302"},"text/plain":"StatementMeta(, 6e945aa3-363c-4cdf-9289-2cbdc3e9199b, 41, Finished, Available)"},"metadata":{}}],"execution_count":39,"metadata":{},"id":"761ed3ff-18fa-41b1-8af0-ecf4f1a8ca7d"},{"cell_type":"code","source":["# Currently this notebook does not support incremental metadata updates. Drop the final metadata table each time this notebook is executed\n","if spark.catalog.tableExists(\"WorkspaceAccessMetadata\"):\n","    drop_stmt = 'DROP TABLE '+myLakehouse+'.WorkspaceAccessMetadata'\n","    result = spark.sql(drop_stmt)\n","    "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"6e945aa3-363c-4cdf-9289-2cbdc3e9199b","statement_id":42,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-04T22:54:35.8926971Z","session_start_time":null,"execution_start_time":"2024-03-04T22:54:39.1220319Z","execution_finish_time":"2024-03-04T22:54:48.9982937Z","parent_msg_id":"ac878450-3f02-436c-9907-4a02edf13da9"},"text/plain":"StatementMeta(, 6e945aa3-363c-4cdf-9289-2cbdc3e9199b, 42, Finished, Available)"},"metadata":{}}],"execution_count":40,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8b6bde5b-e0b0-4d55-b0bc-5e27fe679137"},{"cell_type":"code","source":["# Authentication - Replace string variables with your relevant values      \n","import json, requests, pandas as pd\n","import datetime\n"," \n","try:\n","    from azure.identity import ClientSecretCredential\n","except Exception:\n","     !pip install azure.identity\n","     from azure.identity import ClientSecretCredential\n","\n","authority_url= f'https://login.microsoftonline.com/'\n","#print(authority_url)\n","\n","# Generates the access token for the Service Principal\n","api = 'https://analysis.windows.net/powerbi/api/.default'\n","auth = ClientSecretCredential(authority = authority_url,\n","                              tenant_id = tenant,\n","                              client_id = client,\n","                              client_secret = client_secret)\n","access_token = auth.get_token(api)\n","access_token = access_token.token\n","\n","#print(access_token) \n","print('\\nSuccessfully authenticated.')   "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"6e945aa3-363c-4cdf-9289-2cbdc3e9199b","statement_id":43,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-04T22:54:36.065208Z","session_start_time":null,"execution_start_time":"2024-03-04T22:54:49.3869339Z","execution_finish_time":"2024-03-04T22:54:50.1576234Z","parent_msg_id":"2daba096-4bbd-4d25-b908-2d76f26903a4"},"text/plain":"StatementMeta(, 6e945aa3-363c-4cdf-9289-2cbdc3e9199b, 43, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nSuccessfully authenticated.\n"]}],"execution_count":41,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"dac36c18-7f5d-4e13-9ca1-b3eda3cdce1a"},{"cell_type":"code","source":["# Get workspace ids\n","base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified?excludePersonalWorkspaces=True&excludeInActiveWorkspaces=True'\n","# base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified'\n","# base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n","header = {'Authorization': f'Bearer {access_token}'}\n","\n","response = requests.get(base_url, headers=header)\n","#print(response.content)\n","#print(response['id'][0])\n","data=response.json()\n","df_workpaceids = pd.DataFrame.from_dict(data) \n","display('Workspaceid: '+ df_workpaceids['id'])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"6e945aa3-363c-4cdf-9289-2cbdc3e9199b","statement_id":44,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-04T22:54:36.2806714Z","session_start_time":null,"execution_start_time":"2024-03-04T22:54:50.5037586Z","execution_finish_time":"2024-03-04T22:54:51.325331Z","parent_msg_id":"63173003-7142-4ad0-92ea-99289de87d22"},"text/plain":"StatementMeta(, 6e945aa3-363c-4cdf-9289-2cbdc3e9199b, 44, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0    Workspaceid: 4b63d48a-82f6-4f27-bb75-7a1b8b22df7e\n1    Workspaceid: edf1fd48-b561-4540-82fc-37aed98c4963\n2    Workspaceid: e356b66e-9a99-49a9-886f-4a6055a26644\n3    Workspaceid: 29e068aa-3094-4347-9afd-30c8167ba35e\n4    Workspaceid: bb6157b3-92fb-4995-a2ae-c511bd00b75b\n5    Workspaceid: 6da79f32-7624-4815-a64c-325ea59b39d4\n6    Workspaceid: 988d5ee5-51ff-4c8f-867a-7841d6b96b1e\n7    Workspaceid: 7e8019c5-9879-4ee7-adc6-99ce5081feee\n8    Workspaceid: e26a57cd-fa71-4b01-8ea3-6190d4be4369\nName: id, dtype: object"},"metadata":{}}],"execution_count":42,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"5d3a6efb-df6d-4245-a159-ed29b121f467"},{"cell_type":"code","source":["import time\n","\n","# Loop through all workspace ids\n","for index, row in df_workpaceids.iterrows():\n","    workspaceid = row['id']\n","    print('Workspaceid: ' + workspaceid)\n","\n","    # Trigger workspace scan call for each workspace id\n","    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n","    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n","    payload = json.dumps({\"workspaces\":[workspaceid]})\n","    parametersurl=\"?getArtifactUsers=true\"\n","    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n","    #print(response.text)\n","\n","    # Get the scan id\n","    data=response.json()\n","    df_getInfo = pd.DataFrame.from_dict([data]) \n","    #display(df_getInfo)\n","    #display(df_getInfo['data']['id'])\n","    scanid = df_getInfo['id'][0]\n","    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n","\n","    \n","    # Check if the scan is complete\n","    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n","    header = {\"Authorization\": f'Bearer {access_token}'}\n","    # scanid=df_getInfo['id'][0]\n","    response = requests.get(base_url+scanid, headers=header)\n","    data=response.json()\n","    df_scanStatus = pd.DataFrame.from_dict([data]) \n","    scanStatus = df_scanStatus['status'][0]\n","    display('Status for Scanid ' + scanid + ' for Workspaceid ' + workspaceid + ' is ' +scanStatus) \n","\n","    # keep checking for scan status=succeeded every 30 seconds\n","    while (scanStatus != 'Succeeded'):\n","        time.sleep(30)\n","        response = requests.get(base_url+scanid, headers=header)\n","        data=response.json()\n","        df_scanStatus = pd.DataFrame.from_dict([data]) \n","        scanStatus = df_scanStatus['status'][0]\n","        display('Status for Scanid ' + scanid + ' for Workspaceid ' + workspaceid + ' is ' +scanStatus) \n","\n","    display('Scan for Scanid: '+ scanid + ' is complete')\n","\n","    # Get the metadata for each scan\n","    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanResult/'\n","    header = {\"Authorization\": f'Bearer {access_token}'}\n","    #scanid=df_getInfo['id'][0]\n","\n","    response = requests.get(base_url+scanid, headers=header)\n","\n","    #Convert the JSON data into a dataframe\n","    jsondata = json.loads(json.dumps(response.text))\n","    df_scanResults = spark.read.json(sc.parallelize([jsondata]))\n","    #display(df_scanResults)\n","\n","    # load scan results into multiple views, perform transformations and load final output to a table\n","    df_scanResults.createOrReplaceTempView(\"ScannerAPIoutput_view_0\")\n","    #df_scanResults.write.format(\"delta\").mode(\"append\").save(\"abfss://WS_SagarFabric01@onelake.dfs.fabric.microsoft.com/users01.Lakehouse/Tables/FabricScannerAPI_raw\")\n","\n","    # Explode the content \n","    result = spark.sql(\"CREATE OR REPLACE TEMP VIEW ScannerAPIoutput_view AS select explode(workspaces) AS workspaces from ScannerAPIoutput_view_0\")\n","\n","    result = spark.sql(\"Create or replace TEMP VIEW WorkspaceMetadata_view AS select workspaces.* from ScannerAPIoutput_view\")\n","\n","    df_workspaceName = spark.sql(\"select name from WorkspaceMetadata_view\")\n","    #display(df_workspaceName)\n","    workspaceName = df_workspaceName.first()[0]\n","    #display(workspaceName)\n","\n","    # Include only user defined workspaces\n","    if ('Fabric Capacity Metrics' in workspaceName) or ('Premium Capacity Utilization' in workspaceName) or ('Admin monitoring' in workspaceName):\n","        pass\n","    else:\n","        print ('Valid workspace - do the processing')\n","        #result = spark.sql(\"select * from WorkspaceMetadata_view\")\n","        #display(result)\n","\n","        # check if Notebook metadata is present in the workspace\n","        df_viewcolumns = spark.sql(\"select * from WorkspaceMetadata_view LIMIT 1\")\n","        #display(df_viewcolumns)\n","\n","        # The code below is a series of if statements to look for presence of metadata for all Fabric artifacts such as Notebooks, reports, Lakehouses etc.\n","        # It checks is metadata is present for these artifacts and it is, transforms them and finally loads to a table\n","        if (\"Notebook\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('notebook column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"Notebook\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('notebook column has value')    \n","\n","                    # explode Notebook metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view NotebookMetadata_view AS\n","                                SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                                isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                                explode(Notebook) AS NoteBookMetadata FROM WorkspaceMetadata_view\n","                              \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view NotebookUserAccess_view AS \n","                                Select Notebookmetadata_view.*, NotebookMetadata.name AS ArtifactName, explode(NotebookMetadata.users) as NotebookUserAccess from Notebookmetadata_view\n","                              \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                                SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                                WorkspaceState,'Notebook' AS ArtifactType, ArtifactName, NotebookUserAccess.artifactUserAccessRight AS AccessType,\n","                                NotebookUserAccess.displayName AS UserName, NotebookUserAccess.emailAddress AS UserEmail, NotebookUserAccess.PrincipalType,\n","                                NotebookUserAccess.userType, LastRefreshDatetime from NotebookUserAccess_view          \n","                              \"\"\")   \n","                    #display(result)\n","\n","                    # For the first time, check if the table is present and create it, if not. else insert into the table\n","                    if spark.catalog.tableExists(\"WorkspaceAccessMetadata\"):\n","                        result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","                        print(\"Table present, Data inserted\")\n","                    else:\n","                        # Insert metadata to final table - WorkspaceAccessMetadata\n","                        result.write.format(\"delta\").mode(\"overwrite\").save(myTablePath)\n","                        print(\"Table created\")\n","\n","        if (\"Lakehouse\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('Lakehouse column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"Lakehouse\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('Lakehouse column has value')    \n","\n","                    # explode Lakehouse metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                               create or replace temp view LakehouseMetadata_view AS\n","                               SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                               isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                               explode(Lakehouse) AS LakehouseMetadata FROM WorkspaceMetadata_view\n","                            \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view LakehouseUserAccess_view AS \n","                                Select Lakehousemetadata_view.*, LakehouseMetadata.name AS ArtifactName, explode(LakehouseMetadata.users) as LakehouseUserAccess from Lakehousemetadata_view\n","                            \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                                SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                                WorkspaceState,'Lakehouse' AS ArtifactType, ArtifactName, LakehouseUserAccess.artifactUserAccessRight AS AccessType,\n","                                LakehouseUserAccess.displayName AS UserName, LakehouseUserAccess.emailAddress AS UserEmail, LakehouseUserAccess.PrincipalType,\n","                                LakehouseUserAccess.userType, LastRefreshDatetime from LakehouseUserAccess_view          \n","                            \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"DataPipeline\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('DataPipeline column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"DataPipeline\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('DataPipeline column has value')    \n","            \n","                    # explode DataPipeline metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view DataPipelineMetadata_view AS\n","                                SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                                isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                                explode(DataPipeline) AS DataPipelineMetadata FROM WorkspaceMetadata_view\n","                            \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view DataPipelineUserAccess_view AS \n","                                Select DataPipelinemetadata_view.*, DataPipelineMetadata.name AS ArtifactName, \n","                                explode(DataPipelineMetadata.users) as DataPipelineUserAccess from DataPipelinemetadata_view\n","                            \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                                SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                                WorkspaceState,'DataPipeline' AS ArtifactType, ArtifactName, DataPipelineUserAccess.artifactUserAccessRight AS AccessType,\n","                                DataPipelineUserAccess.displayName AS UserName, DataPipelineUserAccess.emailAddress AS UserEmail, DataPipelineUserAccess.PrincipalType,\n","                                DataPipelineUserAccess.userType, LastRefreshDatetime from DataPipelineUserAccess_view          \n","                            \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"Eventstream\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('Eventstream column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"Eventstream\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('Eventstream column has value')    \n","\n","                    # explode Eventstream metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view EventstreamMetadata_view AS\n","                            SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(Eventstream) AS EventstreamMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view EventstreamUserAccess_view AS \n","                            Select Eventstreammetadata_view.*, EventstreamMetadata.name AS ArtifactName, explode(EventstreamMetadata.users) as EventstreamUserAccess from Eventstreammetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'Eventstream' AS ArtifactType, ArtifactName, EventstreamUserAccess.artifactUserAccessRight AS AccessType,\n","                           EventstreamUserAccess.displayName AS UserName, EventstreamUserAccess.emailAddress AS UserEmail, EventstreamUserAccess.PrincipalType,\n","                           EventstreamUserAccess.userType, LastRefreshDatetime from EventstreamUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"KQLDatabase\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('KQLDatabase column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"KQLDatabase\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('KQLDatabase column has value')    \n","\n","\n","                    # explode KQLDatabase metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view KQLDatabaseMetadata_view AS\n","                                SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                                isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                                explode(KQLDatabase) AS KQLDatabaseMetadata FROM WorkspaceMetadata_view\n","                            \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view KQLDatabaseUserAccess_view AS \n","                            Select KQLDatabasemetadata_view.*, KQLDatabaseMetadata.name AS ArtifactName, explode(KQLDatabaseMetadata.users) as KQLDatabaseUserAccess from KQLDatabasemetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'KQLDatabase' AS ArtifactType, ArtifactName, KQLDatabaseUserAccess.artifactUserAccessRight AS AccessType,\n","                           KQLDatabaseUserAccess.displayName AS UserName, KQLDatabaseUserAccess.emailAddress AS UserEmail, KQLDatabaseUserAccess.PrincipalType,\n","                           KQLDatabaseUserAccess.userType, LastRefreshDatetime from KQLDatabaseUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"KQLQueryset\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('KQLQueryset column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"KQLQueryset\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('KQLQueryset column has value')    \n","\n","                    # explode KQLQueryset metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view KQLQuerysetMetadata_view AS\n","                            SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(KQLQueryset) AS KQLQuerysetMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view KQLQuerysetUserAccess_view AS \n","                            Select KQLQuerysetmetadata_view.*, KQLQuerysetMetadata.name AS ArtifactName, explode(KQLQuerysetMetadata.users) as KQLQuerysetUserAccess from KQLQuerysetmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'KQLQueryset' AS ArtifactType, ArtifactName, KQLQuerysetUserAccess.artifactUserAccessRight AS AccessType,\n","                           KQLQuerysetUserAccess.displayName AS UserName, KQLQuerysetUserAccess.emailAddress AS UserEmail, KQLQuerysetUserAccess.PrincipalType,\n","                           KQLQuerysetUserAccess.userType, LastRefreshDatetime from KQLQuerysetUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"MLExperiment\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('MLExperiment column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"MLExperiment\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('MLExperiment column has value')    \n","\n","                    # explode MLExperiment metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view MLExperimentMetadata_view AS\n","                            SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(MLExperiment) AS MLExperimentMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view MLExperimentUserAccess_view AS \n","                            Select MLExperimentmetadata_view.*, MLExperimentMetadata.name AS ArtifactName, explode(MLExperimentMetadata.users) as MLExperimentUserAccess from MLExperimentmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'MLExperiment' AS ArtifactType, ArtifactName, MLExperimentUserAccess.artifactUserAccessRight AS AccessType,\n","                           MLExperimentUserAccess.displayName AS UserName, MLExperimentUserAccess.emailAddress AS UserEmail, MLExperimentUserAccess.PrincipalType,\n","                           MLExperimentUserAccess.userType, LastRefreshDatetime from MLExperimentUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"MLModel\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('MLModel column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"MLModel\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('MLModel column has value')    \n","\n","                    # explode MLModel metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view MLModelMetadata_view AS\n","                            SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(MLModel) AS MLModelMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view MLModelUserAccess_view AS \n","                            Select MLModelmetadata_view.*, MLModelMetadata.name AS ArtifactName, explode(MLModelMetadata.users) as MLModelUserAccess from MLModelmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'MLModel' AS ArtifactType, ArtifactName, MLModelUserAccess.artifactUserAccessRight AS AccessType,\n","                           MLModelUserAccess.displayName AS UserName, MLModelUserAccess.emailAddress AS UserEmail, MLModelUserAccess.PrincipalType,\n","                           MLModelUserAccess.userType, LastRefreshDatetime from MLModelUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"SQLAnalyticsEndpoint\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('SQLAnalyticsEndpoint column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"SQLAnalyticsEndpoint\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('SQLAnalyticsEndpoint column has value')    \n","\n","                    # explode SQLAnalyticsEndpoint metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view SQLAnalyticsEndpointMetadata_view AS\n","                            SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(SQLAnalyticsEndpoint) AS SQLAnalyticsEndpointMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view SQLAnalyticsEndpointUserAccess_view AS \n","                            Select SQLAnalyticsEndpointmetadata_view.*, SQLAnalyticsEndpointMetadata.name AS ArtifactName, explode(SQLAnalyticsEndpointMetadata.users) as SQLAnalyticsEndpointUserAccess from SQLAnalyticsEndpointmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'SQLAnalyticsEndpoint' AS ArtifactType, ArtifactName, SQLAnalyticsEndpointUserAccess.datamartUserAccessRight AS AccessType,\n","                           SQLAnalyticsEndpointUserAccess.displayName AS UserName, SQLAnalyticsEndpointUserAccess.emailAddress AS UserEmail, SQLAnalyticsEndpointUserAccess.PrincipalType,\n","                           SQLAnalyticsEndpointUserAccess.userType, LastRefreshDatetime from SQLAnalyticsEndpointUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"SparkJobDefinition\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('SparkJobDefinition column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"SparkJobDefinition\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('SparkJobDefinition column has value')    \n","\n","                    # explode SparkJobDefinition metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view SparkJobDefinitionMetadata_view AS\n","                            SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(SparkJobDefinition) AS SparkJobDefinitionMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view SparkJobDefinitionUserAccess_view AS \n","                            Select SparkJobDefinitionmetadata_view.*, SparkJobDefinitionMetadata.name AS ArtifactName, explode(SparkJobDefinitionMetadata.users) as SparkJobDefinitionUserAccess from SparkJobDefinitionmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                            SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                            WorkspaceState,'SparkJobDefinition' AS ArtifactType, ArtifactName, SparkJobDefinitionUserAccess.artifactUserAccessRight AS AccessType,\n","                            SparkJobDefinitionUserAccess.displayName AS UserName, SparkJobDefinitionUserAccess.emailAddress AS UserEmail, SparkJobDefinitionUserAccess.PrincipalType,\n","                            SparkJobDefinitionUserAccess.userType, LastRefreshDatetime from SparkJobDefinitionUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"dashboards\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('dashboards column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"dashboards\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('dashboards column has value')    \n","\n","                    # explode dashboards metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view dashboardsMetadata_view AS\n","                            SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(dashboards) AS dashboardsMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view dashboardsUserAccess_view AS \n","                            Select dashboardsmetadata_view.*, dashboardsMetadata.displayName AS ArtifactName, explode(dashboardsMetadata.users) as dashboardsUserAccess from dashboardsmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'dashboards' AS ArtifactType, ArtifactName, dashboardsUserAccess.dashboardUserAccessRight AS AccessType,\n","                           dashboardsUserAccess.displayName AS UserName, dashboardsUserAccess.emailAddress AS UserEmail, dashboardsUserAccess.PrincipalType,\n","                           dashboardsUserAccess.userType, LastRefreshDatetime from dashboardsUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"dataflows\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('dataflows column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"dataflows\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('dataflows column has value')    \n","\n","                    # explode dataflows metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view dataflowsMetadata_view AS\n","                            SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(dataflows) AS dataflowsMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view dataflowsUserAccess_view AS \n","                            Select dataflowsmetadata_view.*, dataflowsMetadata.name AS ArtifactName, explode(dataflowsMetadata.users) as dataflowsUserAccess from dataflowsmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'dataflows' AS ArtifactType, ArtifactName, dataflowsUserAccess.dataflowUserAccessRight AS AccessType,\n","                           dataflowsUserAccess.displayName AS UserName, dataflowsUserAccess.emailAddress AS UserEmail, dataflowsUserAccess.PrincipalType,\n","                           dataflowsUserAccess.userType, LastRefreshDatetime from dataflowsUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        # Comment datamarts for now until we understand the metadata structure for datamarts\n","        # if (\"datamarts\" not in df_viewcolumns.columns):\n","        #     pass\n","        # else:\n","        #     print('datamarts column present')\n","\n","        #     # explode datamarts metadata\n","        \t\n","        #     result = spark.sql(\"\"\"\n","        #             create or replace temp view datamartsMetadata_view AS\n","        #             SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","        #             isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","        #             explode(datamarts) AS datamartsMetadata FROM WorkspaceMetadata_view\n","        #             \"\"\")\n","\n","        #    if result.isEmpty():\n","        #        pass\n","        #    else:\n","\n","        #        # Create uniform format\n","        #        result = spark.sql(\"\"\"\n","        #                 create or replace temp view datamartsUserAccess_view AS \n","        #                 Select datamartsmetadata_view.*, datamartsMetadata.name AS ArtifactName, explode(datamartsMetadata.users) as datamartsUserAccess from datamartsmetadata_view\n","        #             \"\"\")\n","\n","        #        result = spark.sql(\"\"\"\n","        #             SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","        #                    WorkspaceState,'datamarts' AS ArtifactType, ArtifactName, datamartsUserAccess.datamartUserAccessRight AS AccessType,\n","        #                    datamartsUserAccess.displayName AS UserName, datamartsUserAccess.emailAddress AS UserEmail, datamartsUserAccess.PrincipalType,\n","        #                    datamartsUserAccess.userType from datamartsUserAccess_view          \n","        #             \"\"\")   \n","        #        #display(result)\n","\n","        #        # Insert metadata to final table - WorkspaceAccessMetadata\n","        #        result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"datasets\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('datasets column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"datasets\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('datasets column has value')    \n","\n","                    # explode datasets metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view datasetsMetadata_view AS\n","                            SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(datasets) AS datasetsMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view datasetsUserAccess_view AS \n","                            Select datasetsmetadata_view.*, datasetsMetadata.name AS ArtifactName, explode(datasetsMetadata.users) as datasetsUserAccess from datasetsmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'datasets' AS ArtifactType, ArtifactName, datasetsUserAccess.datasetUserAccessRight AS AccessType,\n","                           datasetsUserAccess.displayName AS UserName, datasetsUserAccess.emailAddress AS UserEmail, datasetsUserAccess.PrincipalType,\n","                           datasetsUserAccess.userType, LastRefreshDatetime from datasetsUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"environment\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('environment column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"environment\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('environment column has value')    \n","\n","                    # explode environment metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view environmentMetadata_view AS\n","                            SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(environment) AS environmentMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view environmentUserAccess_view AS \n","                            Select environmentmetadata_view.*, environmentMetadata.name AS ArtifactName, explode(environmentMetadata.users) as environmentUserAccess from environmentmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'environment' AS ArtifactType, ArtifactName, environmentUserAccess.artifactUserAccessRight AS AccessType,\n","                           environmentUserAccess.displayName AS UserName, environmentUserAccess.emailAddress AS UserEmail, environmentUserAccess.PrincipalType,\n","                           environmentUserAccess.userType, LastRefreshDatetime from environmentUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"kustoeventhubdataconnection\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('kustoeventhubdataconnection column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"kustoeventhubdataconnection\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('kustoeventhubdataconnection column has value')    \n","\n","\n","                    # explode kustoeventhubdataconnection metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view kustoeventhubdataconnectionMetadata_view AS\n","                            SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(kustoeventhubdataconnection) AS kustoeventhubdataconnectionMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view kustoeventhubdataconnectionUserAccess_view AS \n","                            Select kustoeventhubdataconnectionmetadata_view.*, kustoeventhubdataconnectionMetadata.name AS ArtifactName, explode(kustoeventhubdataconnectionMetadata.users) as kustoeventhubdataconnectionUserAccess from kustoeventhubdataconnectionmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'kustoeventhubdataconnection' AS ArtifactType, ArtifactName, kustoeventhubdataconnectionUserAccess.artifactUserAccessRight AS AccessType,\n","                           kustoeventhubdataconnectionUserAccess.displayName AS UserName, kustoeventhubdataconnectionUserAccess.emailAddress AS UserEmail, kustoeventhubdataconnectionUserAccess.PrincipalType,\n","                           kustoeventhubdataconnectionUserAccess.userType, LastRefreshDatetime from kustoeventhubdataconnectionUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"reflexproject\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('reflexproject column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"reflexproject\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('reflexproject column has value')    \n","\n","                    # explode reflexproject metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view reflexprojectMetadata_view AS\n","                            SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(reflexproject) AS reflexprojectMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view reflexprojectUserAccess_view AS \n","                            Select reflexprojectmetadata_view.*, reflexprojectMetadata.name AS ArtifactName, explode(reflexprojectMetadata.users) as reflexprojectUserAccess from reflexprojectmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'reflexproject' AS ArtifactType, ArtifactName, reflexprojectUserAccess.artifactUserAccessRight AS AccessType,\n","                           reflexprojectUserAccess.displayName AS UserName, reflexprojectUserAccess.emailAddress AS UserEmail, reflexprojectUserAccess.PrincipalType,\n","                           reflexprojectUserAccess.userType, LastRefreshDatetime from reflexprojectUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"reports\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('reports column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"reports\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('reports column has value')    \n","\n","                    # explode reports metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view reportsMetadata_view AS\n","                            SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(reports) AS reportsMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view reportsUserAccess_view AS \n","                            Select reportsmetadata_view.*, reportsMetadata.name AS ArtifactName, explode(reportsMetadata.users) as reportsUserAccess from reportsmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'reports' AS ArtifactType, ArtifactName, reportsUserAccess.reportUserAccessRight AS AccessType,\n","                           reportsUserAccess.displayName AS UserName, reportsUserAccess.emailAddress AS UserEmail, reportsUserAccess.PrincipalType,\n","                           reportsUserAccess.userType, LastRefreshDatetime from reportsUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        # if (\"users\" not in df_viewcolumns.columns):\n","        #     pass\n","        # else:\n","        #     print('users column present')\n","\n","        #     # explode users metadata\n","        \t\n","        #     result = spark.sql(\"\"\"\n","        #             create or replace temp view usersMetadata_view AS\n","        #             SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","        #             isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","        #             explode(users) AS usersMetadata FROM WorkspaceMetadata_view\n","        #             \"\"\")\n","\n","        #     # Create uniform format\n","        #     result = spark.sql(\"\"\"\n","        #                 create or replace temp view usersUserAccess_view AS \n","        #                 Select usersmetadata_view.*, usersMetadata.displayName AS ArtifactName, explode(usersMetadata.users) as usersUserAccess from usersmetadata_view\n","        #             \"\"\")\n","\n","        #     result = spark.sql(\"\"\"\n","        #             SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","        #                    WorkspaceState,'users' AS ArtifactType, ArtifactName, usersUserAccess.groupUserAccessRight AS AccessType,\n","        #                    usersUserAccess.displayName AS UserName, usersUserAccess.emailAddress AS UserEmail, usersUserAccess.PrincipalType,\n","        #                    usersUserAccess.userType from usersUserAccess_view          \n","        #             \"\"\")   \n","        #     #display(result)\n","\n","        #     # Insert metadata to final table - WorkspaceAccessMetadata\n","        #     result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"warehouses\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('warehouses column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"warehouses\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('warehouses column has value')    \n","\n","                    # explode warehouses metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view warehousesMetadata_view AS\n","                            SELECT date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss z\") AS LastRefreshDatetime, id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(warehouses) AS warehousesMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view warehousesUserAccess_view AS \n","                            Select warehousesmetadata_view.*, warehousesMetadata.name AS ArtifactName, explode(warehousesMetadata.users) as warehousesUserAccess from warehousesmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'warehouses' AS ArtifactType, ArtifactName, warehousesUserAccess.datamartUserAccessRight AS AccessType,\n","                           warehousesUserAccess.displayName AS UserName, warehousesUserAccess.emailAddress AS UserEmail, warehousesUserAccess.PrincipalType,\n","                           warehousesUserAccess.userType, LastRefreshDatetime from warehousesUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"6e945aa3-363c-4cdf-9289-2cbdc3e9199b","statement_id":45,"state":"finished","livy_statement_state":"available","queued_time":"2024-03-04T22:54:36.6693583Z","session_start_time":null,"execution_start_time":"2024-03-04T22:54:51.7178677Z","execution_finish_time":"2024-03-04T22:57:51.0855038Z","parent_msg_id":"ebae7531-f9e5-406d-b024-2b8fd1183bf9"},"text/plain":"StatementMeta(, 6e945aa3-363c-4cdf-9289-2cbdc3e9199b, 45, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Workspaceid: 4b63d48a-82f6-4f27-bb75-7a1b8b22df7e\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid 4b63d48a-82f6-4f27-bb75-7a1b8b22df7e is fd2749f8-9410-4bf1-b117-acf39ca3c702'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid fd2749f8-9410-4bf1-b117-acf39ca3c702 for Workspaceid 4b63d48a-82f6-4f27-bb75-7a1b8b22df7e is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: fd2749f8-9410-4bf1-b117-acf39ca3c702 is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Valid workspace - do the processing\nnotebook column present\nnotebook column has value\nTable created\nLakehouse column present\nLakehouse column has value\nDataPipeline column present\nDataPipeline column has value\nEventstream column present\nEventstream column has value\nKQLDatabase column present\nKQLDatabase column has value\nKQLQueryset column present\nKQLQueryset column has value\nMLExperiment column present\nMLExperiment column has value\nMLModel column present\nMLModel column has value\nSQLAnalyticsEndpoint column present\nSQLAnalyticsEndpoint column has value\nSparkJobDefinition column present\nSparkJobDefinition column has value\ndashboards column present\ndashboards column has value\ndataflows column present\ndataflows column has value\ndatasets column present\ndatasets column has value\nkustoeventhubdataconnection column present\nkustoeventhubdataconnection column has value\nreports column present\nreports column has value\nwarehouses column present\nwarehouses column has value\nWorkspaceid: edf1fd48-b561-4540-82fc-37aed98c4963\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid edf1fd48-b561-4540-82fc-37aed98c4963 is 2fca3d74-82ce-42cf-9606-c37fc1de79a4'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid 2fca3d74-82ce-42cf-9606-c37fc1de79a4 for Workspaceid edf1fd48-b561-4540-82fc-37aed98c4963 is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: 2fca3d74-82ce-42cf-9606-c37fc1de79a4 is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Valid workspace - do the processing\nnotebook column present\nnotebook column has value\nTable present, Data inserted\nLakehouse column present\nLakehouse column has value\nDataPipeline column present\nDataPipeline column has value\nEventstream column present\nEventstream column has value\nKQLDatabase column present\nKQLDatabase column has value\nMLExperiment column present\nMLExperiment column has value\nMLModel column present\nMLModel column has value\nSQLAnalyticsEndpoint column present\nSQLAnalyticsEndpoint column has value\ndashboards column present\ndashboards column has value\ndataflows column present\ndataflows column has value\ndatasets column present\ndatasets column has value\nreports column present\nreports column has value\nwarehouses column present\nwarehouses column has value\nWorkspaceid: e356b66e-9a99-49a9-886f-4a6055a26644\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid e356b66e-9a99-49a9-886f-4a6055a26644 is 20319ed4-048d-4c4f-8429-3c55934da5eb'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid 20319ed4-048d-4c4f-8429-3c55934da5eb for Workspaceid e356b66e-9a99-49a9-886f-4a6055a26644 is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: 20319ed4-048d-4c4f-8429-3c55934da5eb is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Valid workspace - do the processing\nnotebook column present\nnotebook column has value\nTable present, Data inserted\nLakehouse column present\nLakehouse column has value\nSQLAnalyticsEndpoint column present\nSQLAnalyticsEndpoint column has value\ndashboards column present\ndataflows column present\ndatasets column present\ndatasets column has value\nreports column present\nWorkspaceid: 29e068aa-3094-4347-9afd-30c8167ba35e\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid 29e068aa-3094-4347-9afd-30c8167ba35e is 9c39ef51-691b-4be2-80c7-282ae94f4190'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid 9c39ef51-691b-4be2-80c7-282ae94f4190 for Workspaceid 29e068aa-3094-4347-9afd-30c8167ba35e is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: 9c39ef51-691b-4be2-80c7-282ae94f4190 is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Workspaceid: bb6157b3-92fb-4995-a2ae-c511bd00b75b\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid bb6157b3-92fb-4995-a2ae-c511bd00b75b is 1069799f-c9ef-423a-a34b-27b37951fe1d'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid 1069799f-c9ef-423a-a34b-27b37951fe1d for Workspaceid bb6157b3-92fb-4995-a2ae-c511bd00b75b is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: 1069799f-c9ef-423a-a34b-27b37951fe1d is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Workspaceid: 6da79f32-7624-4815-a64c-325ea59b39d4\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid 6da79f32-7624-4815-a64c-325ea59b39d4 is 98bb1d98-ef04-491a-a80c-96f19e7422d1'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid 98bb1d98-ef04-491a-a80c-96f19e7422d1 for Workspaceid 6da79f32-7624-4815-a64c-325ea59b39d4 is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: 98bb1d98-ef04-491a-a80c-96f19e7422d1 is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Workspaceid: 988d5ee5-51ff-4c8f-867a-7841d6b96b1e\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid 988d5ee5-51ff-4c8f-867a-7841d6b96b1e is fbb9e570-e467-4fab-906d-2900117075ec'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid fbb9e570-e467-4fab-906d-2900117075ec for Workspaceid 988d5ee5-51ff-4c8f-867a-7841d6b96b1e is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: fbb9e570-e467-4fab-906d-2900117075ec is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Valid workspace - do the processing\ndashboards column present\ndataflows column present\ndatasets column present\nreports column present\nWorkspaceid: 7e8019c5-9879-4ee7-adc6-99ce5081feee\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid 7e8019c5-9879-4ee7-adc6-99ce5081feee is f41eb0a4-d86c-463f-9484-ea04239f4560'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid f41eb0a4-d86c-463f-9484-ea04239f4560 for Workspaceid 7e8019c5-9879-4ee7-adc6-99ce5081feee is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: f41eb0a4-d86c-463f-9484-ea04239f4560 is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Valid workspace - do the processing\nnotebook column present\nnotebook column has value\nTable present, Data inserted\nLakehouse column present\nLakehouse column has value\nSQLAnalyticsEndpoint column present\nSQLAnalyticsEndpoint column has value\ndashboards column present\ndataflows column present\ndatasets column present\ndatasets column has value\nreports column present\nWorkspaceid: e26a57cd-fa71-4b01-8ea3-6190d4be4369\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid e26a57cd-fa71-4b01-8ea3-6190d4be4369 is 55d73d23-5e8e-440e-a20e-4458cba1d3cd'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid 55d73d23-5e8e-440e-a20e-4458cba1d3cd for Workspaceid e26a57cd-fa71-4b01-8ea3-6190d4be4369 is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: 55d73d23-5e8e-440e-a20e-4458cba1d3cd is complete'"},"metadata":{}}],"execution_count":43,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"4f9f542c-11b8-442b-9e66-a0c85d348789"},{"cell_type":"markdown","source":["# Change this to code cell when you need to delete data from the Metadata table \n","delete_stmt = 'DELETE FROM '+myLakehouse+'.WorkspaceAccessMetadata'\n","result = spark.sql(delete_stmt)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"36e6efc4-e412-467a-8b96-f0e2e640b245"},{"cell_type":"markdown","source":["%%sql\n","drop table lakehouse01.WorkspaceAccessMetadata"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"64cba1f5-454d-45c3-b035-dbaf81af2b53"},{"cell_type":"markdown","source":["%%sql\n","select * from WorkspaceMetadata_view"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0b9f90a4-379e-4727-a0d2-f4938fb38844"},{"cell_type":"markdown","source":["display(df_scanResults)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8e6510dd-0261-4603-8552-bfd8f6f15ec3"}],"metadata":{"microsoft":{"language":"python"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f"}],"default_lakehouse":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","default_lakehouse_name":"lakehouse01","default_lakehouse_workspace_id":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e"}}},"nbformat":4,"nbformat_minor":5}