{"cells":[{"cell_type":"markdown","source":["# Fabric Scanner API to capture all Fabric artifcats' grants and access information\n","\n","The steps below provide details to have a Service principal read the Fabric metadata. Currently, there is no way in Fabric to show what artifacts are shared with whom. We will capture this metadata via the Scanner APIs\n","\n","https://learn.microsoft.com/en-us/fabric/governance/metadata-scanning-overview\n","\n","Prerequisites:\n","- Follow this documentation to enable SP authentication for read-only Admin APIs\n","    https://learn.microsoft.com/en-us/fabric/admin/metadata-scanning-enable-read-only-apis\n","- \tThe APIs we are going to use are GetModifiedWorkspaces, WorkspacegetInfo, WorkspacescanStatus and WorkspacescanResult\n","- See this documentation for further guidance -\n","\n","    https://learn.microsoft.com/en-us/fabric/admin/metadata-scanning-setup\n","    \n","    https://learn.microsoft.com/en-us/fabric/governance/metadata-scanning-run\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4a6ff3ea-f654-438c-9c9a-f771997693d1"},{"cell_type":"markdown","source":["# Notebook overview\n","\n","This notebook captures Fabric artifact user grants and accesses via Scanner APIs, transforms them into a  standard format (as each artifact's metadata output is not necessarily consistent). We are storing the final output in a table in the lakehouse and create Power BI reports to provide the relevant information\n","\n","Future Enhancements planned:\n","- Divide list of workspace ids in chunks of 100 workspaces and execute each chunk\n","- Implement incremental scan"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2ef315a4-4ca8-419b-ac49-f640763f3ce9"},{"cell_type":"markdown","source":["<mark>**Please ensure notebook setup is done in the next 2 cells before running the notebook**</mark>"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6ec5a646-ff12-4fae-bb26-c331f18f6425"},{"cell_type":"code","source":["# Enter details of workspace, lakehouse where the final table called 'WorkspaceAccessMetadata' should reside in\n","myLakehouse = 'lakehouse01'\n","myWorkspace = 'WS_SagarFabric01'\n","myTablePath = \"abfss://\"+myWorkspace+\"@onelake.dfs.fabric.microsoft.com/\"+myLakehouse+\".Lakehouse/Tables/WorkspaceAccessMetadata\"\n","print(myTablePath)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"bccf7831-2c3d-41c2-9494-2dfa9b07079e","statement_id":12,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-03T20:30:50.7243938Z","session_start_time":null,"execution_start_time":"2024-01-03T20:30:51.1473319Z","execution_finish_time":"2024-01-03T20:30:51.4634657Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"84ac1274-51a0-433c-9297-aadd6d4df7f6"},"text/plain":"StatementMeta(, bccf7831-2c3d-41c2-9494-2dfa9b07079e, 12, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["abfss://WS_SagarFabric01@onelake.dfs.fabric.microsoft.com/lakehouse01.Lakehouse/Tables/WorkspaceAccessMetadata\n"]}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0cb17e88-e851-4501-8b2c-f1f4d7d16f71"},{"cell_type":"code","source":["# Read secrets from Azure Key Vault\n","# Ensure the following items are defined in the keyvault before running the notebook\n","key_vault = \"https://pvlab-9a758f-keyvault.vault.azure.net/\"\n","client_secret = mssparkutils.credentials.getSecret(key_vault , \"secret-app-Fabric-Power-API\")\n","tenant = mssparkutils.credentials.getSecret(key_vault , \"FabricTenantId\")\n","client = mssparkutils.credentials.getSecret(key_vault , \"app-Fabric-Power-API-ClientID\")\n","#print(tenant)\n","#print(client)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"bccf7831-2c3d-41c2-9494-2dfa9b07079e","statement_id":13,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-03T20:30:50.9051063Z","session_start_time":null,"execution_start_time":"2024-01-03T20:30:51.9510775Z","execution_finish_time":"2024-01-03T20:30:52.8113747Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"871ecd7b-9864-422b-b505-81d941846943"},"text/plain":"StatementMeta(, bccf7831-2c3d-41c2-9494-2dfa9b07079e, 13, Finished, Available)"},"metadata":{}}],"execution_count":12,"metadata":{},"id":"761ed3ff-18fa-41b1-8af0-ecf4f1a8ca7d"},{"cell_type":"code","source":["# Currently this notebook does not support incremental metadata updates. Drop the final metadata table each time this notebook is executed\n","if spark.catalog.tableExists(\"WorkspaceAccessMetadata\"):\n","    drop_stmt = 'DROP TABLE '+myLakehouse+'.WorkspaceAccessMetadata'\n","    result = spark.sql(drop_stmt)\n","    "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"bccf7831-2c3d-41c2-9494-2dfa9b07079e","statement_id":14,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-03T20:30:51.1323221Z","session_start_time":null,"execution_start_time":"2024-01-03T20:30:53.2735239Z","execution_finish_time":"2024-01-03T20:31:05.4778056Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":1},"jobs":[{"displayName":"toString at String.java:2951","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":390,"name":"toString at String.java:2951","description":"Job group for statement 14:\n# Currently this notebook does not support incremental metadata updates. Drop the final metadata table each time this notebook is executed\nif spark.catalog.tableExists(\"WorkspaceAccessMetadata\"):\n    drop_stmt = 'DROP TABLE '+myLakehouse+'.WorkspaceAccessMetadata'\n    result = spark.sql(drop_stmt)\n    ","submissionTime":"2024-01-03T20:30:54.517GMT","completionTime":"2024-01-03T20:31:03.601GMT","stageIds":[554],"jobGroup":"14","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"19ad1c9f-1956-40eb-83eb-86c5d5968523"},"text/plain":"StatementMeta(, bccf7831-2c3d-41c2-9494-2dfa9b07079e, 14, Finished, Available)"},"metadata":{}}],"execution_count":13,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8b6bde5b-e0b0-4d55-b0bc-5e27fe679137"},{"cell_type":"code","source":["# Authentication - Replace string variables with your relevant values      \n","import json, requests, pandas as pd\n","import datetime\n"," \n","try:\n","    from azure.identity import ClientSecretCredential\n","except Exception:\n","     !pip install azure.identity\n","     from azure.identity import ClientSecretCredential\n","\n","authority_url= f'https://login.microsoftonline.com/'\n","#print(authority_url)\n","\n","# Generates the access token for the Service Principal\n","api = 'https://analysis.windows.net/powerbi/api/.default'\n","auth = ClientSecretCredential(authority = authority_url,\n","                              tenant_id = tenant,\n","                              client_id = client,\n","                              client_secret = client_secret)\n","access_token = auth.get_token(api)\n","access_token = access_token.token\n","\n","#print(access_token) \n","print('\\nSuccessfully authenticated.')   "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"bccf7831-2c3d-41c2-9494-2dfa9b07079e","statement_id":15,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-03T20:30:51.3799353Z","session_start_time":null,"execution_start_time":"2024-01-03T20:31:05.9934016Z","execution_finish_time":"2024-01-03T20:31:06.2982403Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"3804d99d-81e8-4bea-a835-9fc7270baf81"},"text/plain":"StatementMeta(, bccf7831-2c3d-41c2-9494-2dfa9b07079e, 15, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nSuccessfully authenticated.\n"]}],"execution_count":14,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"dac36c18-7f5d-4e13-9ca1-b3eda3cdce1a"},{"cell_type":"code","source":["# Get workspace ids\n","base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified?excludePersonalWorkspaces=True&excludeInActiveWorkspaces=True'\n","# base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified'\n","# base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n","header = {'Authorization': f'Bearer {access_token}'}\n","\n","response = requests.get(base_url, headers=header)\n","#print(response.content)\n","#print(response['id'][0])\n","data=response.json()\n","df_workpaceids = pd.DataFrame.from_dict(data) \n","display('Workspaceid: '+ df_workpaceids['id'])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"bccf7831-2c3d-41c2-9494-2dfa9b07079e","statement_id":16,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-03T20:30:51.7487915Z","session_start_time":null,"execution_start_time":"2024-01-03T20:31:06.7608365Z","execution_finish_time":"2024-01-03T20:31:07.6450079Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"7a22d95f-9787-4f8e-9ec0-d66101313cd1"},"text/plain":"StatementMeta(, bccf7831-2c3d-41c2-9494-2dfa9b07079e, 16, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0    Workspaceid: 4b63d48a-82f6-4f27-bb75-7a1b8b22df7e\n1    Workspaceid: edf1fd48-b561-4540-82fc-37aed98c4963\n2    Workspaceid: e356b66e-9a99-49a9-886f-4a6055a26644\n3    Workspaceid: 29e068aa-3094-4347-9afd-30c8167ba35e\n4    Workspaceid: bb6157b3-92fb-4995-a2ae-c511bd00b75b\n5    Workspaceid: 6da79f32-7624-4815-a64c-325ea59b39d4\n6    Workspaceid: e26a57cd-fa71-4b01-8ea3-6190d4be4369\nName: id, dtype: object"},"metadata":{}}],"execution_count":15,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"5d3a6efb-df6d-4245-a159-ed29b121f467"},{"cell_type":"code","source":["import time\n","\n","# Loop through all workspace ids\n","for index, row in df_workpaceids.iterrows():\n","    workspaceid = row['id']\n","    print('Workspaceid: ' + workspaceid)\n","\n","    # Trigger workspace scan call for each workspace id\n","    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n","    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n","    payload = json.dumps({\"workspaces\":[workspaceid]})\n","    parametersurl=\"?getArtifactUsers=true\"\n","    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n","    #print(response.text)\n","\n","    # Get the scan id\n","    data=response.json()\n","    df_getInfo = pd.DataFrame.from_dict([data]) \n","    #display(df_getInfo)\n","    #display(df_getInfo['data']['id'])\n","    scanid = df_getInfo['id'][0]\n","    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n","\n","    \n","    # Check if the scan is complete\n","    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n","    header = {\"Authorization\": f'Bearer {access_token}'}\n","    # scanid=df_getInfo['id'][0]\n","    response = requests.get(base_url+scanid, headers=header)\n","    data=response.json()\n","    df_scanStatus = pd.DataFrame.from_dict([data]) \n","    scanStatus = df_scanStatus['status'][0]\n","    display('Status for Scanid ' + scanid + ' for Workspaceid ' + workspaceid + ' is ' +scanStatus) \n","\n","    # keep checking for scan status=succeeded every 30 seconds\n","    while (scanStatus != 'Succeeded'):\n","        time.sleep(30)\n","        response = requests.get(base_url+scanid, headers=header)\n","        data=response.json()\n","        df_scanStatus = pd.DataFrame.from_dict([data]) \n","        scanStatus = df_scanStatus['status'][0]\n","        display('Status for Scanid ' + scanid + ' for Workspaceid ' + workspaceid + ' is ' +scanStatus) \n","\n","    display('Scan for Scanid: '+ scanid + ' is complete')\n","\n","    # Get the metadata for each scan\n","    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanResult/'\n","    header = {\"Authorization\": f'Bearer {access_token}'}\n","    #scanid=df_getInfo['id'][0]\n","\n","    response = requests.get(base_url+scanid, headers=header)\n","\n","    #Convert the JSON data into a dataframe\n","    jsondata = json.loads(json.dumps(response.text))\n","    df_scanResults = spark.read.json(sc.parallelize([jsondata]))\n","    #display(df_scanResults)\n","\n","    # load scan results into multiple views, perform transformations and load final output to a table\n","    df_scanResults.createOrReplaceTempView(\"ScannerAPIoutput_view_0\")\n","    #df_scanResults.write.format(\"delta\").mode(\"append\").save(\"abfss://WS_SagarFabric01@onelake.dfs.fabric.microsoft.com/users01.Lakehouse/Tables/FabricScannerAPI_raw\")\n","\n","    # Explode the content \n","    result = spark.sql(\"CREATE OR REPLACE TEMP VIEW ScannerAPIoutput_view AS select explode(workspaces) AS workspaces from ScannerAPIoutput_view_0\")\n","\n","    result = spark.sql(\"Create or replace TEMP VIEW WorkspaceMetadata_view AS select workspaces.* from ScannerAPIoutput_view\")\n","\n","    df_workspaceName = spark.sql(\"select name from WorkspaceMetadata_view\")\n","    #display(df_workspaceName)\n","    workspaceName = df_workspaceName.first()[0]\n","    #display(workspaceName)\n","\n","    # Include only user defined workspaces\n","    if ('Fabric Capacity Metrics' in workspaceName) or ('Premium Capacity Utilization' in workspaceName) or ('Admin monitoring' in workspaceName):\n","        pass\n","    else:\n","        print ('Valid workspace - do the processing')\n","        #result = spark.sql(\"select * from WorkspaceMetadata_view\")\n","        #display(result)\n","\n","        # check if Notebook metadata is present in the workspace\n","        df_viewcolumns = spark.sql(\"select * from WorkspaceMetadata_view LIMIT 1\")\n","        #display(df_viewcolumns)\n","\n","        # The code below is a series of if statements to look for presence of metadata for all Fabric artifacts such as Notebooks, reports, Lakehouses etc.\n","        # It checks is metadata is present for these artifacts and it is, transforms them and finally loads to a table\n","        if (\"Notebook\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('notebook column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"Notebook\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('notebook column has value')    \n","\n","                    # explode Notebook metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view NotebookMetadata_view AS\n","                                SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                                isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                                explode(Notebook) AS NoteBookMetadata FROM WorkspaceMetadata_view\n","                              \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view NotebookUserAccess_view AS \n","                                Select Notebookmetadata_view.*, NotebookMetadata.name AS ArtifactName, explode(NotebookMetadata.users) as NotebookUserAccess from Notebookmetadata_view\n","                              \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                                SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                                WorkspaceState,'Notebook' AS ArtifactType, ArtifactName, NotebookUserAccess.artifactUserAccessRight AS AccessType,\n","                                NotebookUserAccess.displayName AS UserName, NotebookUserAccess.emailAddress AS UserEmail, NotebookUserAccess.PrincipalType,\n","                                NotebookUserAccess.userType from NotebookUserAccess_view          \n","                              \"\"\")   \n","                    #display(result)\n","\n","                    # For the first time, check if the table is present and create it, if not. else insert into the table\n","                    if spark.catalog.tableExists(\"WorkspaceAccessMetadata\"):\n","                        result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","                        print(\"Table present, Data inserted\")\n","                    else:\n","                        # Insert metadata to final table - WorkspaceAccessMetadata\n","                        result.write.format(\"delta\").mode(\"overwrite\").save(myTablePath)\n","                        print(\"Table created\")\n","\n","        if (\"Lakehouse\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('Lakehouse column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"Lakehouse\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('Lakehouse column has value')    \n","\n","                    # explode Lakehouse metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                               create or replace temp view LakehouseMetadata_view AS\n","                               SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                               isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                               explode(Lakehouse) AS LakehouseMetadata FROM WorkspaceMetadata_view\n","                            \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view LakehouseUserAccess_view AS \n","                                Select Lakehousemetadata_view.*, LakehouseMetadata.name AS ArtifactName, explode(LakehouseMetadata.users) as LakehouseUserAccess from Lakehousemetadata_view\n","                            \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                                SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                                WorkspaceState,'Lakehouse' AS ArtifactType, ArtifactName, LakehouseUserAccess.artifactUserAccessRight AS AccessType,\n","                                LakehouseUserAccess.displayName AS UserName, LakehouseUserAccess.emailAddress AS UserEmail, LakehouseUserAccess.PrincipalType,\n","                                LakehouseUserAccess.userType from LakehouseUserAccess_view          \n","                            \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"DataPipeline\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('DataPipeline column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"DataPipeline\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('DataPipeline column has value')    \n","            \n","                    # explode DataPipeline metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view DataPipelineMetadata_view AS\n","                                SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                                isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                                explode(DataPipeline) AS DataPipelineMetadata FROM WorkspaceMetadata_view\n","                            \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view DataPipelineUserAccess_view AS \n","                                Select DataPipelinemetadata_view.*, DataPipelineMetadata.name AS ArtifactName, \n","                                explode(DataPipelineMetadata.users) as DataPipelineUserAccess from DataPipelinemetadata_view\n","                            \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                                SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                                WorkspaceState,'DataPipeline' AS ArtifactType, ArtifactName, DataPipelineUserAccess.artifactUserAccessRight AS AccessType,\n","                                DataPipelineUserAccess.displayName AS UserName, DataPipelineUserAccess.emailAddress AS UserEmail, DataPipelineUserAccess.PrincipalType,\n","                                DataPipelineUserAccess.userType from DataPipelineUserAccess_view          \n","                            \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"Eventstream\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('Eventstream column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"Eventstream\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('Eventstream column has value')    \n","\n","                    # explode Eventstream metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view EventstreamMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(Eventstream) AS EventstreamMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view EventstreamUserAccess_view AS \n","                            Select Eventstreammetadata_view.*, EventstreamMetadata.name AS ArtifactName, explode(EventstreamMetadata.users) as EventstreamUserAccess from Eventstreammetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'Eventstream' AS ArtifactType, ArtifactName, EventstreamUserAccess.artifactUserAccessRight AS AccessType,\n","                           EventstreamUserAccess.displayName AS UserName, EventstreamUserAccess.emailAddress AS UserEmail, EventstreamUserAccess.PrincipalType,\n","                           EventstreamUserAccess.userType from EventstreamUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"KQLDatabase\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('KQLDatabase column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"KQLDatabase\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('KQLDatabase column has value')    \n","\n","\n","                    # explode KQLDatabase metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view KQLDatabaseMetadata_view AS\n","                                SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                                isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                                explode(KQLDatabase) AS KQLDatabaseMetadata FROM WorkspaceMetadata_view\n","                            \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view KQLDatabaseUserAccess_view AS \n","                            Select KQLDatabasemetadata_view.*, KQLDatabaseMetadata.name AS ArtifactName, explode(KQLDatabaseMetadata.users) as KQLDatabaseUserAccess from KQLDatabasemetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'KQLDatabase' AS ArtifactType, ArtifactName, KQLDatabaseUserAccess.artifactUserAccessRight AS AccessType,\n","                           KQLDatabaseUserAccess.displayName AS UserName, KQLDatabaseUserAccess.emailAddress AS UserEmail, KQLDatabaseUserAccess.PrincipalType,\n","                           KQLDatabaseUserAccess.userType from KQLDatabaseUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"KQLQueryset\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('KQLQueryset column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"KQLQueryset\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('KQLQueryset column has value')    \n","\n","                    # explode KQLQueryset metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view KQLQuerysetMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(KQLQueryset) AS KQLQuerysetMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view KQLQuerysetUserAccess_view AS \n","                            Select KQLQuerysetmetadata_view.*, KQLQuerysetMetadata.name AS ArtifactName, explode(KQLQuerysetMetadata.users) as KQLQuerysetUserAccess from KQLQuerysetmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'KQLQueryset' AS ArtifactType, ArtifactName, KQLQuerysetUserAccess.artifactUserAccessRight AS AccessType,\n","                           KQLQuerysetUserAccess.displayName AS UserName, KQLQuerysetUserAccess.emailAddress AS UserEmail, KQLQuerysetUserAccess.PrincipalType,\n","                           KQLQuerysetUserAccess.userType from KQLQuerysetUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"MLExperiment\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('MLExperiment column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"MLExperiment\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('MLExperiment column has value')    \n","\n","                    # explode MLExperiment metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view MLExperimentMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(MLExperiment) AS MLExperimentMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view MLExperimentUserAccess_view AS \n","                            Select MLExperimentmetadata_view.*, MLExperimentMetadata.name AS ArtifactName, explode(MLExperimentMetadata.users) as MLExperimentUserAccess from MLExperimentmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'MLExperiment' AS ArtifactType, ArtifactName, MLExperimentUserAccess.artifactUserAccessRight AS AccessType,\n","                           MLExperimentUserAccess.displayName AS UserName, MLExperimentUserAccess.emailAddress AS UserEmail, MLExperimentUserAccess.PrincipalType,\n","                           MLExperimentUserAccess.userType from MLExperimentUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"MLModel\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('MLModel column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"MLModel\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('MLModel column has value')    \n","\n","                    # explode MLModel metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view MLModelMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(MLModel) AS MLModelMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view MLModelUserAccess_view AS \n","                            Select MLModelmetadata_view.*, MLModelMetadata.name AS ArtifactName, explode(MLModelMetadata.users) as MLModelUserAccess from MLModelmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'MLModel' AS ArtifactType, ArtifactName, MLModelUserAccess.artifactUserAccessRight AS AccessType,\n","                           MLModelUserAccess.displayName AS UserName, MLModelUserAccess.emailAddress AS UserEmail, MLModelUserAccess.PrincipalType,\n","                           MLModelUserAccess.userType from MLModelUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"SQLAnalyticsEndpoint\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('SQLAnalyticsEndpoint column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"SQLAnalyticsEndpoint\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('SQLAnalyticsEndpoint column has value')    \n","\n","                    # explode SQLAnalyticsEndpoint metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view SQLAnalyticsEndpointMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(SQLAnalyticsEndpoint) AS SQLAnalyticsEndpointMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view SQLAnalyticsEndpointUserAccess_view AS \n","                            Select SQLAnalyticsEndpointmetadata_view.*, SQLAnalyticsEndpointMetadata.name AS ArtifactName, explode(SQLAnalyticsEndpointMetadata.users) as SQLAnalyticsEndpointUserAccess from SQLAnalyticsEndpointmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'SQLAnalyticsEndpoint' AS ArtifactType, ArtifactName, SQLAnalyticsEndpointUserAccess.datamartUserAccessRight AS AccessType,\n","                           SQLAnalyticsEndpointUserAccess.displayName AS UserName, SQLAnalyticsEndpointUserAccess.emailAddress AS UserEmail, SQLAnalyticsEndpointUserAccess.PrincipalType,\n","                           SQLAnalyticsEndpointUserAccess.userType from SQLAnalyticsEndpointUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"SparkJobDefinition\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('SparkJobDefinition column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"SparkJobDefinition\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('SparkJobDefinition column has value')    \n","\n","                    # explode SparkJobDefinition metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view SparkJobDefinitionMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(SparkJobDefinition) AS SparkJobDefinitionMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view SparkJobDefinitionUserAccess_view AS \n","                            Select SparkJobDefinitionmetadata_view.*, SparkJobDefinitionMetadata.name AS ArtifactName, explode(SparkJobDefinitionMetadata.users) as SparkJobDefinitionUserAccess from SparkJobDefinitionmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                            SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                            WorkspaceState,'SparkJobDefinition' AS ArtifactType, ArtifactName, SparkJobDefinitionUserAccess.artifactUserAccessRight AS AccessType,\n","                            SparkJobDefinitionUserAccess.displayName AS UserName, SparkJobDefinitionUserAccess.emailAddress AS UserEmail, SparkJobDefinitionUserAccess.PrincipalType,\n","                            SparkJobDefinitionUserAccess.userType from SparkJobDefinitionUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"dashboards\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('dashboards column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"dashboards\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('dashboards column has value')    \n","\n","                    # explode dashboards metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view dashboardsMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(dashboards) AS dashboardsMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view dashboardsUserAccess_view AS \n","                            Select dashboardsmetadata_view.*, dashboardsMetadata.displayName AS ArtifactName, explode(dashboardsMetadata.users) as dashboardsUserAccess from dashboardsmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'dashboards' AS ArtifactType, ArtifactName, dashboardsUserAccess.dashboardUserAccessRight AS AccessType,\n","                           dashboardsUserAccess.displayName AS UserName, dashboardsUserAccess.emailAddress AS UserEmail, dashboardsUserAccess.PrincipalType,\n","                           dashboardsUserAccess.userType from dashboardsUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"dataflows\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('dataflows column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"dataflows\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('dataflows column has value')    \n","\n","                    # explode dataflows metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view dataflowsMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(dataflows) AS dataflowsMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view dataflowsUserAccess_view AS \n","                            Select dataflowsmetadata_view.*, dataflowsMetadata.name AS ArtifactName, explode(dataflowsMetadata.users) as dataflowsUserAccess from dataflowsmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'dataflows' AS ArtifactType, ArtifactName, dataflowsUserAccess.dataflowUserAccessRight AS AccessType,\n","                           dataflowsUserAccess.displayName AS UserName, dataflowsUserAccess.emailAddress AS UserEmail, dataflowsUserAccess.PrincipalType,\n","                           dataflowsUserAccess.userType from dataflowsUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        # Comment datamarts for now until we understand the metadata structure for datamarts\n","        # if (\"datamarts\" not in df_viewcolumns.columns):\n","        #     pass\n","        # else:\n","        #     print('datamarts column present')\n","\n","        #     # explode datamarts metadata\n","        \t\n","        #     result = spark.sql(\"\"\"\n","        #             create or replace temp view datamartsMetadata_view AS\n","        #             SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","        #             isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","        #             explode(datamarts) AS datamartsMetadata FROM WorkspaceMetadata_view\n","        #             \"\"\")\n","\n","        #    if result.isEmpty():\n","        #        pass\n","        #    else:\n","\n","        #        # Create uniform format\n","        #        result = spark.sql(\"\"\"\n","        #                 create or replace temp view datamartsUserAccess_view AS \n","        #                 Select datamartsmetadata_view.*, datamartsMetadata.name AS ArtifactName, explode(datamartsMetadata.users) as datamartsUserAccess from datamartsmetadata_view\n","        #             \"\"\")\n","\n","        #        result = spark.sql(\"\"\"\n","        #             SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","        #                    WorkspaceState,'datamarts' AS ArtifactType, ArtifactName, datamartsUserAccess.datamartUserAccessRight AS AccessType,\n","        #                    datamartsUserAccess.displayName AS UserName, datamartsUserAccess.emailAddress AS UserEmail, datamartsUserAccess.PrincipalType,\n","        #                    datamartsUserAccess.userType from datamartsUserAccess_view          \n","        #             \"\"\")   \n","        #        #display(result)\n","\n","        #        # Insert metadata to final table - WorkspaceAccessMetadata\n","        #        result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"datasets\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('datasets column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"datasets\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('datasets column has value')    \n","\n","                    # explode datasets metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view datasetsMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(datasets) AS datasetsMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view datasetsUserAccess_view AS \n","                            Select datasetsmetadata_view.*, datasetsMetadata.name AS ArtifactName, explode(datasetsMetadata.users) as datasetsUserAccess from datasetsmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'datasets' AS ArtifactType, ArtifactName, datasetsUserAccess.datasetUserAccessRight AS AccessType,\n","                           datasetsUserAccess.displayName AS UserName, datasetsUserAccess.emailAddress AS UserEmail, datasetsUserAccess.PrincipalType,\n","                           datasetsUserAccess.userType from datasetsUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"environment\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('environment column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"environment\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('environment column has value')    \n","\n","                    # explode environment metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view environmentMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(environment) AS environmentMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view environmentUserAccess_view AS \n","                            Select environmentmetadata_view.*, environmentMetadata.name AS ArtifactName, explode(environmentMetadata.users) as environmentUserAccess from environmentmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'environment' AS ArtifactType, ArtifactName, environmentUserAccess.artifactUserAccessRight AS AccessType,\n","                           environmentUserAccess.displayName AS UserName, environmentUserAccess.emailAddress AS UserEmail, environmentUserAccess.PrincipalType,\n","                           environmentUserAccess.userType from environmentUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"kustoeventhubdataconnection\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('kustoeventhubdataconnection column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"kustoeventhubdataconnection\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('kustoeventhubdataconnection column has value')    \n","\n","\n","                    # explode kustoeventhubdataconnection metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view kustoeventhubdataconnectionMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(kustoeventhubdataconnection) AS kustoeventhubdataconnectionMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view kustoeventhubdataconnectionUserAccess_view AS \n","                            Select kustoeventhubdataconnectionmetadata_view.*, kustoeventhubdataconnectionMetadata.name AS ArtifactName, explode(kustoeventhubdataconnectionMetadata.users) as kustoeventhubdataconnectionUserAccess from kustoeventhubdataconnectionmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'kustoeventhubdataconnection' AS ArtifactType, ArtifactName, kustoeventhubdataconnectionUserAccess.artifactUserAccessRight AS AccessType,\n","                           kustoeventhubdataconnectionUserAccess.displayName AS UserName, kustoeventhubdataconnectionUserAccess.emailAddress AS UserEmail, kustoeventhubdataconnectionUserAccess.PrincipalType,\n","                           kustoeventhubdataconnectionUserAccess.userType from kustoeventhubdataconnectionUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"reflexproject\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('reflexproject column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"reflexproject\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('reflexproject column has value')    \n","\n","                    # explode reflexproject metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view reflexprojectMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(reflexproject) AS reflexprojectMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view reflexprojectUserAccess_view AS \n","                            Select reflexprojectmetadata_view.*, reflexprojectMetadata.name AS ArtifactName, explode(reflexprojectMetadata.users) as reflexprojectUserAccess from reflexprojectmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'reflexproject' AS ArtifactType, ArtifactName, reflexprojectUserAccess.artifactUserAccessRight AS AccessType,\n","                           reflexprojectUserAccess.displayName AS UserName, reflexprojectUserAccess.emailAddress AS UserEmail, reflexprojectUserAccess.PrincipalType,\n","                           reflexprojectUserAccess.userType from reflexprojectUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"reports\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('reports column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"reports\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('reports column has value')    \n","\n","                    # explode reports metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view reportsMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(reports) AS reportsMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view reportsUserAccess_view AS \n","                            Select reportsmetadata_view.*, reportsMetadata.name AS ArtifactName, explode(reportsMetadata.users) as reportsUserAccess from reportsmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'reports' AS ArtifactType, ArtifactName, reportsUserAccess.reportUserAccessRight AS AccessType,\n","                           reportsUserAccess.displayName AS UserName, reportsUserAccess.emailAddress AS UserEmail, reportsUserAccess.PrincipalType,\n","                           reportsUserAccess.userType from reportsUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"users\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('users column present')\n","\n","        # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"users\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('users column has value')        \n","\n","                # explode workspace users metadata\n","        \t\n","                result = spark.sql(\"\"\"\n","                    create or replace temp view WorkspaceUsersMetadata_view AS\n","                    SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                    isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                    explode(users) AS WorkspaceUsersMetadata FROM WorkspaceMetadata_view;\n","                    \"\"\")\n","\n","                result = spark.sql(\"\"\"\n","                        SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                        WorkspaceState,'Workspace' AS ArtifactType, WorkspaceName AS ArtifactName, WorkspaceUsersMetadata.groupUserAccessRight AS AccessType,\n","                        WorkspaceUsersMetadata.displayName AS UserName, WorkspaceUsersMetadata.emailAddress AS UserEmail, WorkspaceUsersMetadata.PrincipalType,\n","                        WorkspaceUsersMetadata.userType from WorkspaceUsersMetadata_view            \n","                    \"\"\")   \n","                #display(result)\n","\n","                # Insert metadata to final table - WorkspaceAccessMetadata\n","                result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"warehouses\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('warehouses column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"warehouses\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('warehouses column has value')    \n","\n","                    # explode warehouses metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view warehousesMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(warehouses) AS warehousesMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view warehousesUserAccess_view AS \n","                            Select warehousesmetadata_view.*, warehousesMetadata.name AS ArtifactName, explode(warehousesMetadata.users) as warehousesUserAccess from warehousesmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'warehouses' AS ArtifactType, ArtifactName, warehousesUserAccess.datamartUserAccessRight AS AccessType,\n","                           warehousesUserAccess.displayName AS UserName, warehousesUserAccess.emailAddress AS UserEmail, warehousesUserAccess.PrincipalType,\n","                           warehousesUserAccess.userType from warehousesUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"bccf7831-2c3d-41c2-9494-2dfa9b07079e","statement_id":17,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-03T20:30:52.1581538Z","session_start_time":null,"execution_start_time":"2024-01-03T20:31:08.1745695Z","execution_finish_time":"2024-01-03T20:33:41.889864Z","spark_jobs":{"numbers":{"RUNNING":1,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":158},"jobs":[{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":549,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...: Compute snapshot for version: 16","submissionTime":"2024-01-03T20:32:20.199GMT","stageIds":[780],"jobGroup":"17","status":"RUNNING","numTasks":7,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2951","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":548,"name":"toString at String.java:2951","description":"Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...","submissionTime":"2024-01-03T20:32:19.976GMT","completionTime":"2024-01-03T20:32:20.026GMT","stageIds":[779],"jobGroup":"17","status":"SUCCEEDED","numTasks":7,"numActiveTasks":0,"numCompletedTasks":7,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":7,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at <unknown>:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":547,"name":"save at <unknown>:0","description":"Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...","submissionTime":"2024-01-03T20:32:18.354GMT","completionTime":"2024-01-03T20:32:18.634GMT","stageIds":[778,777],"jobGroup":"17","status":"SUCCEEDED","numTasks":17,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":16,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at <unknown>:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":546,"name":"save at <unknown>:0","description":"Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...","submissionTime":"2024-01-03T20:32:18.013GMT","completionTime":"2024-01-03T20:32:18.317GMT","stageIds":[776],"jobGroup":"17","status":"SUCCEEDED","numTasks":16,"numActiveTasks":0,"numCompletedTasks":16,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":16,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_6304/1256128825.py:777","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":545,"name":"collect at /tmp/ipykernel_6304/1256128825.py:777","description":"Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...","submissionTime":"2024-01-03T20:32:17.616GMT","completionTime":"2024-01-03T20:32:17.832GMT","stageIds":[775],"jobGroup":"17","status":"SUCCEEDED","numTasks":11,"numActiveTasks":0,"numCompletedTasks":11,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":11,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_6304/1256128825.py:777","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":544,"name":"collect at /tmp/ipykernel_6304/1256128825.py:777","description":"Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...","submissionTime":"2024-01-03T20:32:17.492GMT","completionTime":"2024-01-03T20:32:17.612GMT","stageIds":[774],"jobGroup":"17","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_6304/1256128825.py:777","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":543,"name":"collect at /tmp/ipykernel_6304/1256128825.py:777","description":"Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...","submissionTime":"2024-01-03T20:32:17.399GMT","completionTime":"2024-01-03T20:32:17.488GMT","stageIds":[773],"jobGroup":"17","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":542,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...: Compute snapshot for version: 15","submissionTime":"2024-01-03T20:32:17.354GMT","completionTime":"2024-01-03T20:32:17.381GMT","stageIds":[771,772,770],"jobGroup":"17","status":"SUCCEEDED","numTasks":57,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":56,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":541,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...: Compute snapshot for version: 15","submissionTime":"2024-01-03T20:32:17.127GMT","completionTime":"2024-01-03T20:32:17.341GMT","stageIds":[768,769],"jobGroup":"17","status":"SUCCEEDED","numTasks":56,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":6,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":540,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...: Compute snapshot for version: 15","submissionTime":"2024-01-03T20:32:16.969GMT","completionTime":"2024-01-03T20:32:17.030GMT","stageIds":[767],"jobGroup":"17","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2951","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":539,"name":"toString at String.java:2951","description":"Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...","submissionTime":"2024-01-03T20:32:16.749GMT","completionTime":"2024-01-03T20:32:16.802GMT","stageIds":[766],"jobGroup":"17","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at <unknown>:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":538,"name":"save at <unknown>:0","description":"Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...","submissionTime":"2024-01-03T20:32:15.191GMT","completionTime":"2024-01-03T20:32:15.428GMT","stageIds":[764,765],"jobGroup":"17","status":"SUCCEEDED","numTasks":17,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":16,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at <unknown>:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":537,"name":"save at <unknown>:0","description":"Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...","submissionTime":"2024-01-03T20:32:14.855GMT","completionTime":"2024-01-03T20:32:15.160GMT","stageIds":[763],"jobGroup":"17","status":"SUCCEEDED","numTasks":16,"numActiveTasks":0,"numCompletedTasks":16,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":16,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_6304/1256128825.py:737","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":536,"name":"collect at /tmp/ipykernel_6304/1256128825.py:737","description":"Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...","submissionTime":"2024-01-03T20:32:14.457GMT","completionTime":"2024-01-03T20:32:14.671GMT","stageIds":[762],"jobGroup":"17","status":"SUCCEEDED","numTasks":11,"numActiveTasks":0,"numCompletedTasks":11,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":11,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_6304/1256128825.py:737","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":535,"name":"collect at /tmp/ipykernel_6304/1256128825.py:737","description":"Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...","submissionTime":"2024-01-03T20:32:14.330GMT","completionTime":"2024-01-03T20:32:14.453GMT","stageIds":[761],"jobGroup":"17","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"collect at /tmp/ipykernel_6304/1256128825.py:737","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":534,"name":"collect at /tmp/ipykernel_6304/1256128825.py:737","description":"Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...","submissionTime":"2024-01-03T20:32:14.221GMT","completionTime":"2024-01-03T20:32:14.326GMT","stageIds":[760],"jobGroup":"17","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":533,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...: Compute snapshot for version: 14","submissionTime":"2024-01-03T20:32:14.174GMT","completionTime":"2024-01-03T20:32:14.200GMT","stageIds":[757,758,759],"jobGroup":"17","status":"SUCCEEDED","numTasks":56,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":55,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":532,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...: Compute snapshot for version: 14","submissionTime":"2024-01-03T20:32:13.960GMT","completionTime":"2024-01-03T20:32:14.161GMT","stageIds":[755,756],"jobGroup":"17","status":"SUCCEEDED","numTasks":55,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":531,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:107","description":"Delta: Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...: Compute snapshot for version: 14","submissionTime":"2024-01-03T20:32:13.813GMT","completionTime":"2024-01-03T20:32:13.871GMT","stageIds":[754],"jobGroup":"17","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":5,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":5,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2951","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":530,"name":"toString at String.java:2951","description":"Job group for statement 17:\nimport time\n\n# Loop through all workspace ids\nfor index, row in df_workpaceids.iterrows():\n    workspaceid = row['id']\n    print('Workspaceid: ' + workspaceid)\n\n    # Trigger workspace scan call for each workspace id\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n    payload = json.dumps({\"workspaces\":[workspaceid]})\n    parametersurl=\"?getArtifactUsers=true\"\n    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n    #print(response.text)\n\n    # Get the scan id\n    data=response.json()\n    df_getInfo = pd.DataFrame.from_dict([data]) \n    #display(df_getInfo)\n    #display(df_getInfo['data']['id'])\n    scanid = df_getInfo['id'][0]\n    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n\n    \n    # Check if the scan is complete\n    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n    header = {\"Authorization\": f...","submissionTime":"2024-01-03T20:32:13.582GMT","completionTime":"2024-01-03T20:32:13.638GMT","stageIds":[753],"jobGroup":"17","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":5,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":5,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"23632fba-522f-4694-ade8-3b4bad5fc50c"},"text/plain":"StatementMeta(, bccf7831-2c3d-41c2-9494-2dfa9b07079e, 17, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Workspaceid: 4b63d48a-82f6-4f27-bb75-7a1b8b22df7e\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid 4b63d48a-82f6-4f27-bb75-7a1b8b22df7e is fcd569fa-bc8b-4edd-92c8-9d1b75df91a8'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid fcd569fa-bc8b-4edd-92c8-9d1b75df91a8 for Workspaceid 4b63d48a-82f6-4f27-bb75-7a1b8b22df7e is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: fcd569fa-bc8b-4edd-92c8-9d1b75df91a8 is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Valid workspace - do the processing\nnotebook column present\nnotebook column has value\nTable created\nLakehouse column present\nLakehouse column has value\nDataPipeline column present\nDataPipeline column has value\nEventstream column present\nEventstream column has value\nKQLDatabase column present\nKQLDatabase column has value\nKQLQueryset column present\nKQLQueryset column has value\nMLExperiment column present\nMLExperiment column has value\nMLModel column present\nMLModel column has value\nSQLAnalyticsEndpoint column present\nSQLAnalyticsEndpoint column has value\nSparkJobDefinition column present\nSparkJobDefinition column has value\ndashboards column present\ndashboards column has value\ndataflows column present\ndataflows column has value\ndatasets column present\ndatasets column has value\nenvironment column present\nenvironment column has value\nkustoeventhubdataconnection column present\nkustoeventhubdataconnection column has value\nreflexproject column present\nreflexproject column has value\nreports column present\nreports column has value\nusers column present\nusers column has value\nwarehouses column present\nwarehouses column has value\nWorkspaceid: edf1fd48-b561-4540-82fc-37aed98c4963\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid edf1fd48-b561-4540-82fc-37aed98c4963 is a57cd77f-e7f4-4c6f-a1c9-8b40db0d31b6'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid a57cd77f-e7f4-4c6f-a1c9-8b40db0d31b6 for Workspaceid edf1fd48-b561-4540-82fc-37aed98c4963 is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: a57cd77f-e7f4-4c6f-a1c9-8b40db0d31b6 is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Valid workspace - do the processing\nnotebook column present\nnotebook column has value\nTable present, Data inserted\nLakehouse column present\nLakehouse column has value\nEventstream column present\nEventstream column has value\nKQLDatabase column present\nKQLDatabase column has value\nMLExperiment column present\nMLExperiment column has value\nMLModel column present\nMLModel column has value\nSQLAnalyticsEndpoint column present\nSQLAnalyticsEndpoint column has value\ndashboards column present\ndashboards column has value\ndataflows column present\ndataflows column has value\ndatasets column present\ndatasets column has value\nreflexproject column present\nreflexproject column has value\nreports column present\nreports column has value\nusers column present\nusers column has value\nwarehouses column present\nwarehouses column has value\nWorkspaceid: e356b66e-9a99-49a9-886f-4a6055a26644\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid e356b66e-9a99-49a9-886f-4a6055a26644 is 55505cad-3c88-4368-8ae0-a8f8009538d2'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid 55505cad-3c88-4368-8ae0-a8f8009538d2 for Workspaceid e356b66e-9a99-49a9-886f-4a6055a26644 is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: 55505cad-3c88-4368-8ae0-a8f8009538d2 is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Valid workspace - do the processing\nnotebook column present\nnotebook column has value\nTable present, Data inserted\nLakehouse column present\nLakehouse column has value\nSQLAnalyticsEndpoint column present\nSQLAnalyticsEndpoint column has value\ndashboards column present\ndataflows column present\ndatasets column present\ndatasets column has value\nreports column present\nusers column present\nusers column has value\nWorkspaceid: 29e068aa-3094-4347-9afd-30c8167ba35e\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid 29e068aa-3094-4347-9afd-30c8167ba35e is ec2d6d30-69b2-4bab-9eff-64fe5417e218'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid ec2d6d30-69b2-4bab-9eff-64fe5417e218 for Workspaceid 29e068aa-3094-4347-9afd-30c8167ba35e is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: ec2d6d30-69b2-4bab-9eff-64fe5417e218 is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Workspaceid: bb6157b3-92fb-4995-a2ae-c511bd00b75b\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid bb6157b3-92fb-4995-a2ae-c511bd00b75b is 991289a8-aaaf-45bb-be81-7aadcfd9d0d6'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid 991289a8-aaaf-45bb-be81-7aadcfd9d0d6 for Workspaceid bb6157b3-92fb-4995-a2ae-c511bd00b75b is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: 991289a8-aaaf-45bb-be81-7aadcfd9d0d6 is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Workspaceid: 6da79f32-7624-4815-a64c-325ea59b39d4\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid 6da79f32-7624-4815-a64c-325ea59b39d4 is 5e56f5b6-21ca-4465-ae7a-e955e86fee29'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid 5e56f5b6-21ca-4465-ae7a-e955e86fee29 for Workspaceid 6da79f32-7624-4815-a64c-325ea59b39d4 is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: 5e56f5b6-21ca-4465-ae7a-e955e86fee29 is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Workspaceid: e26a57cd-fa71-4b01-8ea3-6190d4be4369\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid e26a57cd-fa71-4b01-8ea3-6190d4be4369 is e2036571-9130-41d7-8137-b11d0e542828'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid e2036571-9130-41d7-8137-b11d0e542828 for Workspaceid e26a57cd-fa71-4b01-8ea3-6190d4be4369 is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: e2036571-9130-41d7-8137-b11d0e542828 is complete'"},"metadata":{}}],"execution_count":16,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"4f9f542c-11b8-442b-9e66-a0c85d348789"},{"cell_type":"markdown","source":["# Change this to code cell when you need to delete data from the Metadata table \n","delete_stmt = 'DELETE FROM '+myLakehouse+'.WorkspaceAccessMetadata'\n","result = spark.sql(delete_stmt)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"36e6efc4-e412-467a-8b96-f0e2e640b245"},{"cell_type":"markdown","source":["%%sql\n","drop table lakehouse01.WorkspaceAccessMetadata"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"64cba1f5-454d-45c3-b035-dbaf81af2b53"},{"cell_type":"markdown","source":["%%sql\n","select * from WorkspaceMetadata_view"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0b9f90a4-379e-4727-a0d2-f4938fb38844"},{"cell_type":"markdown","source":["display(df_scanResults)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8e6510dd-0261-4603-8552-bfd8f6f15ec3"}],"metadata":{"microsoft":{"host":{"synapse_widget":{"token":"38b55a93-e39a-4396-a59a-68258ea3bee5","state":{"6a5843fe-6419-4454-8f64-0c2ca264ee67":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":[{"name":"Admin monitoring","dashboards":[],"state":"Active","description":"Admin monitoring","users":[{"displayName":"Admin Monitoring","identifier":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","emailAddress":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","groupUserAccessRight":"Admin","graphId":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","principalType":"User"},{"displayName":"Tenant Administrators","identifier":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","groupUserAccessRight":"None","graphId":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","principalType":"Group"}],"dataflows":[],"datasets":[{"name":"Feature Usage and Adoption","configuredBy":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","users":[{"displayName":"Admin Monitoring","identifier":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","emailAddress":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","graphId":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","datasetUserAccessRight":"ReadWriteReshareExplore","principalType":"User"},{"displayName":"Tenant Administrators","identifier":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","graphId":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","datasetUserAccessRight":"ReadReshareExplore","principalType":"Group"}],"isEffectiveIdentityRequired":false,"targetStorageMode":"Abf","id":"d50acc54-5c7a-4be5-a00b-ad265be391f5","isEffectiveIdentityRolesRequired":false,"configuredById":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","tables":[],"refreshSchedule":{"enabled":true,"localTimeZoneId":"UTC","times":["15:29"],"notifyOption":"NoNotification","days":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]},"contentProviderType":"PbixInImportMode","createdDate":"2023-06-21T13:47:56.367"},{"name":"Purview Hub","configuredBy":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","users":[{"displayName":"Admin Monitoring","identifier":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","emailAddress":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","graphId":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","datasetUserAccessRight":"ReadWriteReshareExplore","principalType":"User"},{"displayName":"Tenant Administrators","identifier":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","graphId":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","datasetUserAccessRight":"ReadReshareExplore","principalType":"Group"}],"isEffectiveIdentityRequired":false,"targetStorageMode":"Abf","id":"cdead5e9-678c-4039-b702-0988da6ecd4a","isEffectiveIdentityRolesRequired":false,"configuredById":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","tables":[],"refreshSchedule":{"enabled":true,"localTimeZoneId":"UTC","times":["16:59"],"notifyOption":"MailOnFailure","days":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]},"contentProviderType":"PbixInImportMode","createdDate":"2023-06-21T13:48:02.253"}],"datamarts":[],"id":"e26a57cd-fa71-4b01-8ea3-6190d4be4369","reports":[{"createdById":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","name":"Feature Usage and Adoption","users":[{"displayName":"Admin Monitoring","identifier":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","emailAddress":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","graphId":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","principalType":"User","reportUserAccessRight":"Owner"},{"displayName":"Tenant Administrators","identifier":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","graphId":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","principalType":"Group","reportUserAccessRight":"ReadReshareExplore"}],"modifiedById":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","datasetId":"d50acc54-5c7a-4be5-a00b-ad265be391f5","id":"a1f9456b-9c74-4dd3-bcb8-271c8fad3fcb","reportType":"PowerBIReport","createdDateTime":"2023-06-21T13:47:57.57","createdBy":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","modifiedDateTime":"2023-12-14T15:24:07.96","modifiedBy":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537"},{"createdById":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","name":"Purview Hub","users":[{"displayName":"Admin Monitoring","identifier":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","emailAddress":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","graphId":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","principalType":"User","reportUserAccessRight":"Owner"},{"displayName":"Tenant Administrators","identifier":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","graphId":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","principalType":"Group","reportUserAccessRight":"ReadReshareExplore"}],"modifiedById":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","datasetId":"cdead5e9-678c-4039-b702-0988da6ecd4a","id":"7958c102-921a-4aa4-8a95-eeaa89a0f268","reportType":"PowerBIReport","createdDateTime":"2023-06-21T13:48:03.053","createdBy":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","modifiedDateTime":"2023-08-11T16:53:59.34","modifiedBy":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537"}],"type":"AdminWorkspace","isOnDedicatedCapacity":false}],"index":1}],"schema":[{"key":"0","name":"workspaces","type":"ArrayType(StructType(StructField(dashboards,ArrayType(StringType,true),true),StructField(dataflows,ArrayType(StringType,true),true),StructField(datamarts,ArrayType(StringType,true),true),StructField(datasets,ArrayType(StructType(StructField(configuredBy,StringType,true),StructField(configuredById,StringType,true),StructField(contentProviderType,StringType,true),StructField(createdDate,StringType,true),StructField(id,StringType,true),StructField(isEffectiveIdentityRequired,BooleanType,true),StructField(isEffectiveIdentityRolesRequired,BooleanType,true),StructField(name,StringType,true),StructField(refreshSchedule,StructType(StructField(days,ArrayType(StringType,true),true),StructField(enabled,BooleanType,true),StructField(localTimeZoneId,StringType,true),StructField(notifyOption,StringType,true),StructField(times,ArrayType(StringType,true),true)),true),StructField(tables,ArrayType(StringType,true),true),StructField(targetStorageMode,StringType,true),StructField(users,ArrayType(StructType(StructField(datasetUserAccessRight,StringType,true),StructField(displayName,StringType,true),StructField(emailAddress,StringType,true),StructField(graphId,StringType,true),StructField(identifier,StringType,true),StructField(principalType,StringType,true),StructField(userType,StringType,true)),true),true)),true),true),StructField(description,StringType,true),StructField(id,StringType,true),StructField(isOnDedicatedCapacity,BooleanType,true),StructField(name,StringType,true),StructField(reports,ArrayType(StructType(StructField(createdBy,StringType,true),StructField(createdById,StringType,true),StructField(createdDateTime,StringType,true),StructField(datasetId,StringType,true),StructField(id,StringType,true),StructField(modifiedBy,StringType,true),StructField(modifiedById,StringType,true),StructField(modifiedDateTime,StringType,true),StructField(name,StringType,true),StructField(reportType,StringType,true),StructField(users,ArrayType(StructType(StructField(displayName,StringType,true),StructField(emailAddress,StringType,true),StructField(graphId,StringType,true),StructField(identifier,StringType,true),StructField(principalType,StringType,true),StructField(reportUserAccessRight,StringType,true),StructField(userType,StringType,true)),true),true)),true),true),StructField(state,StringType,true),StructField(type,StringType,true),StructField(users,ArrayType(StructType(StructField(displayName,StringType,true),StructField(emailAddress,StringType,true),StructField(graphId,StringType,true),StructField(groupUserAccessRight,StringType,true),StructField(identifier,StringType,true),StructField(principalType,StringType,true),StructField(userType,StringType,true)),true),true)),true)"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}}},"language":"python"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{"6a5843fe-6419-4454-8f64-0c2ca264ee67":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":[{"name":"Admin monitoring","dashboards":[],"state":"Active","description":"Admin monitoring","users":[{"displayName":"Admin Monitoring","identifier":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","emailAddress":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","groupUserAccessRight":"Admin","graphId":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","principalType":"User"},{"displayName":"Tenant Administrators","identifier":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","groupUserAccessRight":"None","graphId":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","principalType":"Group"}],"dataflows":[],"datasets":[{"name":"Feature Usage and Adoption","configuredBy":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","users":[{"displayName":"Admin Monitoring","identifier":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","emailAddress":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","graphId":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","datasetUserAccessRight":"ReadWriteReshareExplore","principalType":"User"},{"displayName":"Tenant Administrators","identifier":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","graphId":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","datasetUserAccessRight":"ReadReshareExplore","principalType":"Group"}],"isEffectiveIdentityRequired":false,"targetStorageMode":"Abf","id":"d50acc54-5c7a-4be5-a00b-ad265be391f5","isEffectiveIdentityRolesRequired":false,"configuredById":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","tables":[],"refreshSchedule":{"enabled":true,"localTimeZoneId":"UTC","times":["15:29"],"notifyOption":"NoNotification","days":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]},"contentProviderType":"PbixInImportMode","createdDate":"2023-06-21T13:47:56.367"},{"name":"Purview Hub","configuredBy":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","users":[{"displayName":"Admin Monitoring","identifier":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","emailAddress":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","graphId":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","datasetUserAccessRight":"ReadWriteReshareExplore","principalType":"User"},{"displayName":"Tenant Administrators","identifier":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","graphId":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","datasetUserAccessRight":"ReadReshareExplore","principalType":"Group"}],"isEffectiveIdentityRequired":false,"targetStorageMode":"Abf","id":"cdead5e9-678c-4039-b702-0988da6ecd4a","isEffectiveIdentityRolesRequired":false,"configuredById":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","tables":[],"refreshSchedule":{"enabled":true,"localTimeZoneId":"UTC","times":["16:59"],"notifyOption":"MailOnFailure","days":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]},"contentProviderType":"PbixInImportMode","createdDate":"2023-06-21T13:48:02.253"}],"datamarts":[],"id":"e26a57cd-fa71-4b01-8ea3-6190d4be4369","reports":[{"createdById":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","name":"Feature Usage and Adoption","users":[{"displayName":"Admin Monitoring","identifier":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","emailAddress":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","graphId":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","principalType":"User","reportUserAccessRight":"Owner"},{"displayName":"Tenant Administrators","identifier":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","graphId":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","principalType":"Group","reportUserAccessRight":"ReadReshareExplore"}],"modifiedById":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","datasetId":"d50acc54-5c7a-4be5-a00b-ad265be391f5","id":"a1f9456b-9c74-4dd3-bcb8-271c8fad3fcb","reportType":"PowerBIReport","createdDateTime":"2023-06-21T13:47:57.57","createdBy":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","modifiedDateTime":"2023-12-14T15:24:07.96","modifiedBy":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537"},{"createdById":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","name":"Purview Hub","users":[{"displayName":"Admin Monitoring","identifier":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","emailAddress":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","graphId":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","principalType":"User","reportUserAccessRight":"Owner"},{"displayName":"Tenant Administrators","identifier":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","graphId":"ada3d3f6-d9f9-4b3e-b75a-94dfd8969623","principalType":"Group","reportUserAccessRight":"ReadReshareExplore"}],"modifiedById":"c25cc8d9-88bc-42c1-bf7c-c66d133e4537","datasetId":"cdead5e9-678c-4039-b702-0988da6ecd4a","id":"7958c102-921a-4aa4-8a95-eeaa89a0f268","reportType":"PowerBIReport","createdDateTime":"2023-06-21T13:48:03.053","createdBy":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537","modifiedDateTime":"2023-08-11T16:53:59.34","modifiedBy":"AdminInsights-c25cc8d9-88bc-42c1-bf7c-c66d133e4537"}],"type":"AdminWorkspace","isOnDedicatedCapacity":false}],"index":1}],"schema":[{"key":"0","name":"workspaces","type":"ArrayType(StructType(StructField(dashboards,ArrayType(StringType,true),true),StructField(dataflows,ArrayType(StringType,true),true),StructField(datamarts,ArrayType(StringType,true),true),StructField(datasets,ArrayType(StructType(StructField(configuredBy,StringType,true),StructField(configuredById,StringType,true),StructField(contentProviderType,StringType,true),StructField(createdDate,StringType,true),StructField(id,StringType,true),StructField(isEffectiveIdentityRequired,BooleanType,true),StructField(isEffectiveIdentityRolesRequired,BooleanType,true),StructField(name,StringType,true),StructField(refreshSchedule,StructType(StructField(days,ArrayType(StringType,true),true),StructField(enabled,BooleanType,true),StructField(localTimeZoneId,StringType,true),StructField(notifyOption,StringType,true),StructField(times,ArrayType(StringType,true),true)),true),StructField(tables,ArrayType(StringType,true),true),StructField(targetStorageMode,StringType,true),StructField(users,ArrayType(StructType(StructField(datasetUserAccessRight,StringType,true),StructField(displayName,StringType,true),StructField(emailAddress,StringType,true),StructField(graphId,StringType,true),StructField(identifier,StringType,true),StructField(principalType,StringType,true),StructField(userType,StringType,true)),true),true)),true),true),StructField(description,StringType,true),StructField(id,StringType,true),StructField(isOnDedicatedCapacity,BooleanType,true),StructField(name,StringType,true),StructField(reports,ArrayType(StructType(StructField(createdBy,StringType,true),StructField(createdById,StringType,true),StructField(createdDateTime,StringType,true),StructField(datasetId,StringType,true),StructField(id,StringType,true),StructField(modifiedBy,StringType,true),StructField(modifiedById,StringType,true),StructField(modifiedDateTime,StringType,true),StructField(name,StringType,true),StructField(reportType,StringType,true),StructField(users,ArrayType(StructType(StructField(displayName,StringType,true),StructField(emailAddress,StringType,true),StructField(graphId,StringType,true),StructField(identifier,StringType,true),StructField(principalType,StringType,true),StructField(reportUserAccessRight,StringType,true),StructField(userType,StringType,true)),true),true)),true),true),StructField(state,StringType,true),StructField(type,StringType,true),StructField(users,ArrayType(StructType(StructField(displayName,StringType,true),StructField(emailAddress,StringType,true),StructField(graphId,StringType,true),StructField(groupUserAccessRight,StringType,true),StructField(identifier,StringType,true),StructField(principalType,StringType,true),StructField(userType,StringType,true)),true),true)),true)"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f"}],"default_lakehouse":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","default_lakehouse_name":"lakehouse01","default_lakehouse_workspace_id":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e"}}},"nbformat":4,"nbformat_minor":5}