{"cells":[{"cell_type":"markdown","source":["# Fabric Scanner API to capture all Fabric artifcats grants and access information\n","\n","The steps below provide details to have a Service principal read the Fabric metadata. Currently, there is no way in Fabric to show what artifacts are shared with whom. We will capture this metadata via the Scanner APIs\n","\n","https://learn.microsoft.com/en-us/fabric/governance/metadata-scanning-overview\n","\n","Prerequisites:\n","- Follow this documentation to enable SP authentication for read-only Admin APIs\n","    https://learn.microsoft.com/en-us/fabric/admin/metadata-scanning-enable-read-only-apis\n","- \tThe APIs we are going to use are GetModifiedWorkspaces, WorkspacegetInfo, WorkspacescanStatus and WorkspacescanResult\n","- See this documentation for further guidance -\n","\n","    https://learn.microsoft.com/en-us/fabric/admin/metadata-scanning-setup\n","    \n","    https://learn.microsoft.com/en-us/fabric/governance/metadata-scanning-run\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4a6ff3ea-f654-438c-9c9a-f771997693d1"},{"cell_type":"markdown","source":["# Notebook overview\n","\n","This notebook captures Fabric artifact user grants and accesses via Scanner APIs, transforms them into a  standard format (as each artifact's metadata output is not necessarily consistent). We are storing the final output in a table in the lakehouse and create Power BI reports to provide the relevant information\n","\n","Future Enhancements planned:\n","- Divide list of workspace ids in chunks of 100 workspaces and execute each chunk\n","- Implement incremental scan"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2ef315a4-4ca8-419b-ac49-f640763f3ce9"},{"cell_type":"code","source":["# Enter details of workspace, lakehouse where the final table called 'WorkspaceAccessMetadata' should reside in\n","myLakehouse = 'lakehouse01'\n","myWorkspace = 'WS_SagarFabric01'\n","myTablePath = \"abfss://\"+myWorkspace+\"@onelake.dfs.fabric.microsoft.com/\"+myLakehouse+\".Lakehouse/Tables/WorkspaceAccessMetadata\"\n","print(myTablePath)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"805ff07c-7edf-4e93-b7fc-c75c6a3c70c4","statement_id":23,"state":"finished","livy_statement_state":"available","queued_time":"2023-12-22T16:17:55.2456585Z","session_start_time":null,"execution_start_time":"2023-12-22T16:17:55.9740317Z","execution_finish_time":"2023-12-22T16:17:56.3050107Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":0,"RUNNING":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"2ac52f15-5e57-42a2-980f-364f50d88ce1"},"text/plain":"StatementMeta(, 805ff07c-7edf-4e93-b7fc-c75c6a3c70c4, 23, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["abfss://WS_SagarFabric01@onelake.dfs.fabric.microsoft.com/lakehouse01.Lakehouse/Tables/WorkspaceAccessMetadata\n"]}],"execution_count":21,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0cb17e88-e851-4501-8b2c-f1f4d7d16f71"},{"cell_type":"code","source":["# Read secrets from Azure Key Vault\n","# Ensure the following items are defined in the keyvault before running the notebook\n","key_vault = \"https://pvlab-9a758f-keyvault.vault.azure.net/\"\n","client_secret = mssparkutils.credentials.getSecret(key_vault , \"secret-app-Fabric-Power-API\")\n","tenant = mssparkutils.credentials.getSecret(key_vault , \"FabricTenantId\")\n","client = mssparkutils.credentials.getSecret(key_vault , \"app-Fabric-Power-API-ClientID\")\n","#print(tenant)\n","#print(client)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"805ff07c-7edf-4e93-b7fc-c75c6a3c70c4","statement_id":24,"state":"finished","livy_statement_state":"available","queued_time":"2023-12-22T16:17:55.4040718Z","session_start_time":null,"execution_start_time":"2023-12-22T16:17:56.779978Z","execution_finish_time":"2023-12-22T16:17:57.6981473Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":0,"RUNNING":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"aaffcae7-1fe0-4e59-bf0e-e944a5e4356b"},"text/plain":"StatementMeta(, 805ff07c-7edf-4e93-b7fc-c75c6a3c70c4, 24, Finished, Available)"},"metadata":{}}],"execution_count":22,"metadata":{},"id":"761ed3ff-18fa-41b1-8af0-ecf4f1a8ca7d"},{"cell_type":"code","source":["# Authentication - Replace string variables with your relevant values      \n","import json, requests, pandas as pd\n","import datetime\n"," \n","try:\n","    from azure.identity import ClientSecretCredential\n","except Exception:\n","     !pip install azure.identity\n","     from azure.identity import ClientSecretCredential\n","\n","authority_url= f'https://login.microsoftonline.com/'\n","#print(authority_url)\n","\n","# Generates the access token for the Service Principal\n","api = 'https://analysis.windows.net/powerbi/api/.default'\n","auth = ClientSecretCredential(authority = authority_url,\n","                              tenant_id = tenant,\n","                              client_id = client,\n","                              client_secret = client_secret)\n","access_token = auth.get_token(api)\n","access_token = access_token.token\n","\n","#print(access_token) \n","print('\\nSuccessfully authenticated.')   "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"805ff07c-7edf-4e93-b7fc-c75c6a3c70c4","statement_id":25,"state":"finished","livy_statement_state":"available","queued_time":"2023-12-22T16:17:55.6843787Z","session_start_time":null,"execution_start_time":"2023-12-22T16:17:58.1147954Z","execution_finish_time":"2023-12-22T16:17:58.3834496Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":0,"RUNNING":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"31976ca9-492b-4b24-abc0-1b3f9da47c3e"},"text/plain":"StatementMeta(, 805ff07c-7edf-4e93-b7fc-c75c6a3c70c4, 25, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nSuccessfully authenticated.\n"]}],"execution_count":23,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"dac36c18-7f5d-4e13-9ca1-b3eda3cdce1a"},{"cell_type":"code","source":["# Get workspace ids\n","base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified?excludePersonalWorkspaces=True&excludeInActiveWorkspaces=True'\n","# base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified'\n","# base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n","header = {'Authorization': f'Bearer {access_token}'}\n","\n","response = requests.get(base_url, headers=header)\n","#print(response.content)\n","#print(response['id'][0])\n","data=response.json()\n","df_workpaceids = pd.DataFrame.from_dict(data) \n","display('Workspaceid: '+ df_workpaceids['id'])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"805ff07c-7edf-4e93-b7fc-c75c6a3c70c4","statement_id":26,"state":"finished","livy_statement_state":"available","queued_time":"2023-12-22T16:17:55.9743272Z","session_start_time":null,"execution_start_time":"2023-12-22T16:17:58.8107923Z","execution_finish_time":"2023-12-22T16:17:59.1414489Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":0,"RUNNING":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"036d6fc1-bb75-4f8e-9c36-2be52bd9f3c5"},"text/plain":"StatementMeta(, 805ff07c-7edf-4e93-b7fc-c75c6a3c70c4, 26, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0    Workspaceid: 4b63d48a-82f6-4f27-bb75-7a1b8b22df7e\n1    Workspaceid: edf1fd48-b561-4540-82fc-37aed98c4963\n2    Workspaceid: e356b66e-9a99-49a9-886f-4a6055a26644\n3    Workspaceid: 29e068aa-3094-4347-9afd-30c8167ba35e\n4    Workspaceid: bb6157b3-92fb-4995-a2ae-c511bd00b75b\n5    Workspaceid: 6da79f32-7624-4815-a64c-325ea59b39d4\n6    Workspaceid: e26a57cd-fa71-4b01-8ea3-6190d4be4369\nName: id, dtype: object"},"metadata":{}}],"execution_count":24,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"5d3a6efb-df6d-4245-a159-ed29b121f467"},{"cell_type":"code","source":["import time\n","\n","# Loop through all workspace ids\n","for index, row in df_workpaceids.iterrows():\n","    workspaceid = row['id']\n","    print('Workspaceid: ' + workspaceid)\n","\n","    # Trigger workspace scan call for each workspace id\n","    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo'\n","    header = {\"Content-Type\": \"application/json\",\"Authorization\": f'Bearer {access_token}'}\n","    payload = json.dumps({\"workspaces\":[workspaceid]})\n","    parametersurl=\"?getArtifactUsers=true\"\n","    response = requests.request(\"POST\", base_url+parametersurl, headers=header, data=payload)\n","    #print(response.text)\n","\n","    # Get the scan id\n","    data=response.json()\n","    df_getInfo = pd.DataFrame.from_dict([data]) \n","    #display(df_getInfo)\n","    #display(df_getInfo['data']['id'])\n","    scanid = df_getInfo['id'][0]\n","    display('Scanid for Workspaceid ' + workspaceid + ' is ' +scanid) \n","\n","    \n","    # Check if the scan is complete\n","    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/'\n","    header = {\"Authorization\": f'Bearer {access_token}'}\n","    # scanid=df_getInfo['id'][0]\n","    response = requests.get(base_url+scanid, headers=header)\n","    data=response.json()\n","    df_scanStatus = pd.DataFrame.from_dict([data]) \n","    scanStatus = df_scanStatus['status'][0]\n","    display('Status for Scanid ' + scanid + ' for Workspaceid ' + workspaceid + ' is ' +scanStatus) \n","\n","    # keep checking for scan status=succeeded every 30 seconds\n","    while (scanStatus != 'Succeeded'):\n","        time.sleep(30)\n","        response = requests.get(base_url+scanid, headers=header)\n","        data=response.json()\n","        df_scanStatus = pd.DataFrame.from_dict([data]) \n","        scanStatus = df_scanStatus['status'][0]\n","        display('Status for Scanid ' + scanid + ' for Workspaceid ' + workspaceid + ' is ' +scanStatus) \n","\n","    display('Scan for Scanid: '+ scanid + ' is complete')\n","\n","    # Get the metadata for each scan\n","    base_url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanResult/'\n","    header = {\"Authorization\": f'Bearer {access_token}'}\n","    #scanid=df_getInfo['id'][0]\n","\n","    response = requests.get(base_url+scanid, headers=header)\n","\n","    #Convert the JSON data into a dataframe\n","    jsondata = json.loads(json.dumps(response.text))\n","    df_scanResults = spark.read.json(sc.parallelize([jsondata]))\n","    #display(df_scanResults)\n","\n","    # load scan results into multiple views, perform transformations and load final output to a table\n","    df_scanResults.createOrReplaceTempView(\"ScannerAPIoutput_view_0\")\n","    #df_scanResults.write.format(\"delta\").mode(\"append\").save(\"abfss://WS_SagarFabric01@onelake.dfs.fabric.microsoft.com/users01.Lakehouse/Tables/FabricScannerAPI_raw\")\n","\n","    # Explode the content \n","    result = spark.sql(\"CREATE OR REPLACE TEMP VIEW ScannerAPIoutput_view AS select explode(workspaces) AS workspaces from ScannerAPIoutput_view_0\")\n","\n","    result = spark.sql(\"Create or replace TEMP VIEW WorkspaceMetadata_view AS select workspaces.* from ScannerAPIoutput_view\")\n","\n","    df_workspaceName = spark.sql(\"select name from WorkspaceMetadata_view\")\n","    #display(df_workspaceName)\n","    workspaceName = df_workspaceName.first()[0]\n","    #display(workspaceName)\n","\n","    # Include only user defined workspaces\n","    if ('Fabric Capacity Metrics' in workspaceName) or ('Premium Capacity Utilization' in workspaceName) or ('Admin monitoring' in workspaceName):\n","        pass\n","    else:\n","        print ('Valid workspace - do the processing')\n","        #result = spark.sql(\"select * from WorkspaceMetadata_view\")\n","        #display(result)\n","\n","        # check if Notebook metadata is present in the workspace\n","        df_viewcolumns = spark.sql(\"select * from WorkspaceMetadata_view LIMIT 1\")\n","        #display(df_viewcolumns)\n","\n","        # The code below is a series of if statements to look for presence of metadata for all Fabric artifacts such as Notebooks, reports, Lakehouses etc.\n","        # It checks is metadata is present for these artifacts and it is, transforms them and finally loads to a table\n","        if (\"Notebook\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('notebook column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"Notebook\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('notebook column has value')    \n","\n","                    # explode Notebook metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view NotebookMetadata_view AS\n","                                SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                                domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                                explode(Notebook) AS NoteBookMetadata FROM WorkspaceMetadata_view\n","                              \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view NotebookUserAccess_view AS \n","                                Select Notebookmetadata_view.*, NotebookMetadata.name AS ArtifactName, explode(NotebookMetadata.users) as NotebookUserAccess from Notebookmetadata_view\n","                              \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                                SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                                WorkspaceState,'Notebook' AS ArtifactType, ArtifactName, NotebookUserAccess.artifactUserAccessRight AS AccessType,\n","                                NotebookUserAccess.displayName AS UserName, NotebookUserAccess.emailAddress AS UserEmail, NotebookUserAccess.PrincipalType,\n","                                NotebookUserAccess.userType from NotebookUserAccess_view          \n","                              \"\"\")   \n","                    #display(result)\n","\n","                    # For the first time, check if the table is present and create it, if not. else insert into the table\n","                    if spark.catalog.tableExists(\"WorkspaceAccessMetadata\"):\n","                        result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","                        print(\"Table present, Data inserted\")\n","                    else:\n","                        # Insert metadata to final table - WorkspaceAccessMetadata\n","                        result.write.format(\"delta\").mode(\"overwrite\").save(myTablePath)\n","                        print(\"Table created\")\n","\n","        if (\"Lakehouse\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('Lakehouse column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"Lakehouse\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('Lakehouse column has value')    \n","\n","                    # explode Lakehouse metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                               create or replace temp view LakehouseMetadata_view AS\n","                               SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                               domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                               explode(Lakehouse) AS LakehouseMetadata FROM WorkspaceMetadata_view\n","                            \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view LakehouseUserAccess_view AS \n","                                Select Lakehousemetadata_view.*, LakehouseMetadata.name AS ArtifactName, explode(LakehouseMetadata.users) as LakehouseUserAccess from Lakehousemetadata_view\n","                            \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                                SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                                WorkspaceState,'Lakehouse' AS ArtifactType, ArtifactName, LakehouseUserAccess.artifactUserAccessRight AS AccessType,\n","                                LakehouseUserAccess.displayName AS UserName, LakehouseUserAccess.emailAddress AS UserEmail, LakehouseUserAccess.PrincipalType,\n","                                LakehouseUserAccess.userType from LakehouseUserAccess_view          \n","                            \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"DataPipeline\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('DataPipeline column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"Lakehouse\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('DataPipeline column has value')    \n","            \n","                    # explode DataPipeline metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view DataPipelineMetadata_view AS\n","                                SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                                domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                                explode(DataPipeline) AS DataPipelineMetadata FROM WorkspaceMetadata_view\n","                            \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view DataPipelineUserAccess_view AS \n","                                Select DataPipelinemetadata_view.*, DataPipelineMetadata.name AS ArtifactName, \n","                                explode(DataPipelineMetadata.users) as DataPipelineUserAccess from DataPipelinemetadata_view\n","                            \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                                SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                                WorkspaceState,'DataPipeline' AS ArtifactType, ArtifactName, DataPipelineUserAccess.artifactUserAccessRight AS AccessType,\n","                                DataPipelineUserAccess.displayName AS UserName, DataPipelineUserAccess.emailAddress AS UserEmail, DataPipelineUserAccess.PrincipalType,\n","                                DataPipelineUserAccess.userType from DataPipelineUserAccess_view          \n","                            \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"Eventstream\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('Eventstream column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"Eventstream\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('Eventstream column has value')    \n","\n","                    # explode Eventstream metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view EventstreamMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(Eventstream) AS EventstreamMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view EventstreamUserAccess_view AS \n","                            Select Eventstreammetadata_view.*, EventstreamMetadata.name AS ArtifactName, explode(EventstreamMetadata.users) as EventstreamUserAccess from Eventstreammetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'Eventstream' AS ArtifactType, ArtifactName, EventstreamUserAccess.artifactUserAccessRight AS AccessType,\n","                           EventstreamUserAccess.displayName AS UserName, EventstreamUserAccess.emailAddress AS UserEmail, EventstreamUserAccess.PrincipalType,\n","                           EventstreamUserAccess.userType from EventstreamUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"KQLDatabase\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('KQLDatabase column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"KQLDatabase\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('KQLDatabase column has value')    \n","\n","\n","                    # explode KQLDatabase metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                                create or replace temp view KQLDatabaseMetadata_view AS\n","                                SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                                domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                                explode(KQLDatabase) AS KQLDatabaseMetadata FROM WorkspaceMetadata_view\n","                            \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view KQLDatabaseUserAccess_view AS \n","                            Select KQLDatabasemetadata_view.*, KQLDatabaseMetadata.name AS ArtifactName, explode(KQLDatabaseMetadata.users) as KQLDatabaseUserAccess from KQLDatabasemetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'KQLDatabase' AS ArtifactType, ArtifactName, KQLDatabaseUserAccess.artifactUserAccessRight AS AccessType,\n","                           KQLDatabaseUserAccess.displayName AS UserName, KQLDatabaseUserAccess.emailAddress AS UserEmail, KQLDatabaseUserAccess.PrincipalType,\n","                           KQLDatabaseUserAccess.userType from KQLDatabaseUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"KQLQueryset\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('KQLQueryset column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"KQLQueryset\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('KQLQueryset column has value')    \n","\n","                    # explode KQLQueryset metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view KQLQuerysetMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(KQLQueryset) AS KQLQuerysetMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view KQLQuerysetUserAccess_view AS \n","                            Select KQLQuerysetmetadata_view.*, KQLQuerysetMetadata.name AS ArtifactName, explode(KQLQuerysetMetadata.users) as KQLQuerysetUserAccess from KQLQuerysetmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'KQLQueryset' AS ArtifactType, ArtifactName, KQLQuerysetUserAccess.artifactUserAccessRight AS AccessType,\n","                           KQLQuerysetUserAccess.displayName AS UserName, KQLQuerysetUserAccess.emailAddress AS UserEmail, KQLQuerysetUserAccess.PrincipalType,\n","                           KQLQuerysetUserAccess.userType from KQLQuerysetUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"MLExperiment\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('MLExperiment column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"MLExperiment\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('MLExperiment column has value')    \n","\n","                    # explode MLExperiment metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view MLExperimentMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(MLExperiment) AS MLExperimentMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view MLExperimentUserAccess_view AS \n","                            Select MLExperimentmetadata_view.*, MLExperimentMetadata.name AS ArtifactName, explode(MLExperimentMetadata.users) as MLExperimentUserAccess from MLExperimentmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'MLExperiment' AS ArtifactType, ArtifactName, MLExperimentUserAccess.artifactUserAccessRight AS AccessType,\n","                           MLExperimentUserAccess.displayName AS UserName, MLExperimentUserAccess.emailAddress AS UserEmail, MLExperimentUserAccess.PrincipalType,\n","                           MLExperimentUserAccess.userType from MLExperimentUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"MLModel\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('MLModel column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"MLModel\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('MLModel column has value')    \n","\n","                    # explode MLModel metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view MLModelMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(MLModel) AS MLModelMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view MLModelUserAccess_view AS \n","                            Select MLModelmetadata_view.*, MLModelMetadata.name AS ArtifactName, explode(MLModelMetadata.users) as MLModelUserAccess from MLModelmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'MLModel' AS ArtifactType, ArtifactName, MLModelUserAccess.artifactUserAccessRight AS AccessType,\n","                           MLModelUserAccess.displayName AS UserName, MLModelUserAccess.emailAddress AS UserEmail, MLModelUserAccess.PrincipalType,\n","                           MLModelUserAccess.userType from MLModelUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"SQLAnalyticsEndpoint\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('SQLAnalyticsEndpoint column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"SQLAnalyticsEndpoint\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('SQLAnalyticsEndpoint column has value')    \n","\n","                    # explode SQLAnalyticsEndpoint metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view SQLAnalyticsEndpointMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(SQLAnalyticsEndpoint) AS SQLAnalyticsEndpointMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view SQLAnalyticsEndpointUserAccess_view AS \n","                            Select SQLAnalyticsEndpointmetadata_view.*, SQLAnalyticsEndpointMetadata.name AS ArtifactName, explode(SQLAnalyticsEndpointMetadata.users) as SQLAnalyticsEndpointUserAccess from SQLAnalyticsEndpointmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'SQLAnalyticsEndpoint' AS ArtifactType, ArtifactName, SQLAnalyticsEndpointUserAccess.datamartUserAccessRight AS AccessType,\n","                           SQLAnalyticsEndpointUserAccess.displayName AS UserName, SQLAnalyticsEndpointUserAccess.emailAddress AS UserEmail, SQLAnalyticsEndpointUserAccess.PrincipalType,\n","                           SQLAnalyticsEndpointUserAccess.userType from SQLAnalyticsEndpointUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"SparkJobDefinition\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('SparkJobDefinition column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"SparkJobDefinition\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('SparkJobDefinition column has value')    \n","\n","                    # explode SparkJobDefinition metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view SparkJobDefinitionMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(SparkJobDefinition) AS SparkJobDefinitionMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view SparkJobDefinitionUserAccess_view AS \n","                            Select SparkJobDefinitionmetadata_view.*, SparkJobDefinitionMetadata.name AS ArtifactName, explode(SparkJobDefinitionMetadata.users) as SparkJobDefinitionUserAccess from SparkJobDefinitionmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                            SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                            WorkspaceState,'SparkJobDefinition' AS ArtifactType, ArtifactName, SparkJobDefinitionUserAccess.artifactUserAccessRight AS AccessType,\n","                            SparkJobDefinitionUserAccess.displayName AS UserName, SparkJobDefinitionUserAccess.emailAddress AS UserEmail, SparkJobDefinitionUserAccess.PrincipalType,\n","                            SparkJobDefinitionUserAccess.userType from SparkJobDefinitionUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"dashboards\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('dashboards column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"dashboards\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('dashboards column has value')    \n","\n","                    # explode dashboards metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view dashboardsMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(dashboards) AS dashboardsMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view dashboardsUserAccess_view AS \n","                            Select dashboardsmetadata_view.*, dashboardsMetadata.displayName AS ArtifactName, explode(dashboardsMetadata.users) as dashboardsUserAccess from dashboardsmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'dashboards' AS ArtifactType, ArtifactName, dashboardsUserAccess.dashboardUserAccessRight AS AccessType,\n","                           dashboardsUserAccess.displayName AS UserName, dashboardsUserAccess.emailAddress AS UserEmail, dashboardsUserAccess.PrincipalType,\n","                           dashboardsUserAccess.userType from dashboardsUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"dataflows\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('dataflows column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"dataflows\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('dataflows column has value')    \n","\n","                    # explode dataflows metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view dataflowsMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(dataflows) AS dataflowsMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view dataflowsUserAccess_view AS \n","                            Select dataflowsmetadata_view.*, dataflowsMetadata.name AS ArtifactName, explode(dataflowsMetadata.users) as dataflowsUserAccess from dataflowsmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'dataflows' AS ArtifactType, ArtifactName, dataflowsUserAccess.dataflowUserAccessRight AS AccessType,\n","                           dataflowsUserAccess.displayName AS UserName, dataflowsUserAccess.emailAddress AS UserEmail, dataflowsUserAccess.PrincipalType,\n","                           dataflowsUserAccess.userType from dataflowsUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        # Comment datamarts for now until we understand the metadata structure for datamarts\n","        # if (\"datamarts\" not in df_viewcolumns.columns):\n","        #     pass\n","        # else:\n","        #     print('datamarts column present')\n","\n","        #     # explode datamarts metadata\n","        \t\n","        #     result = spark.sql(\"\"\"\n","        #             create or replace temp view datamartsMetadata_view AS\n","        #             SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","        #             domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","        #             explode(datamarts) AS datamartsMetadata FROM WorkspaceMetadata_view\n","        #             \"\"\")\n","\n","        #    if result.isEmpty():\n","        #        pass\n","        #    else:\n","\n","        #        # Create uniform format\n","        #        result = spark.sql(\"\"\"\n","        #                 create or replace temp view datamartsUserAccess_view AS \n","        #                 Select datamartsmetadata_view.*, datamartsMetadata.name AS ArtifactName, explode(datamartsMetadata.users) as datamartsUserAccess from datamartsmetadata_view\n","        #             \"\"\")\n","\n","        #        result = spark.sql(\"\"\"\n","        #             SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","        #                    WorkspaceState,'datamarts' AS ArtifactType, ArtifactName, datamartsUserAccess.datamartUserAccessRight AS AccessType,\n","        #                    datamartsUserAccess.displayName AS UserName, datamartsUserAccess.emailAddress AS UserEmail, datamartsUserAccess.PrincipalType,\n","        #                    datamartsUserAccess.userType from datamartsUserAccess_view          \n","        #             \"\"\")   \n","        #        #display(result)\n","\n","        #        # Insert metadata to final table - WorkspaceAccessMetadata\n","        #        result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"datasets\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('datasets column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"datasets\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('datasets column has value')    \n","\n","                    # explode datasets metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view datasetsMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(datasets) AS datasetsMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view datasetsUserAccess_view AS \n","                            Select datasetsmetadata_view.*, datasetsMetadata.name AS ArtifactName, explode(datasetsMetadata.users) as datasetsUserAccess from datasetsmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'datasets' AS ArtifactType, ArtifactName, datasetsUserAccess.datasetUserAccessRight AS AccessType,\n","                           datasetsUserAccess.displayName AS UserName, datasetsUserAccess.emailAddress AS UserEmail, datasetsUserAccess.PrincipalType,\n","                           datasetsUserAccess.userType from datasetsUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"environment\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('environment column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"environment\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('environment column has value')    \n","\n","                    # explode environment metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view environmentMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(environment) AS environmentMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view environmentUserAccess_view AS \n","                            Select environmentmetadata_view.*, environmentMetadata.name AS ArtifactName, explode(environmentMetadata.users) as environmentUserAccess from environmentmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'environment' AS ArtifactType, ArtifactName, environmentUserAccess.artifactUserAccessRight AS AccessType,\n","                           environmentUserAccess.displayName AS UserName, environmentUserAccess.emailAddress AS UserEmail, environmentUserAccess.PrincipalType,\n","                           environmentUserAccess.userType from environmentUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"kustoeventhubdataconnection\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('kustoeventhubdataconnection column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"kustoeventhubdataconnection\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('kustoeventhubdataconnection column has value')    \n","\n","\n","                    # explode kustoeventhubdataconnection metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view kustoeventhubdataconnectionMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(kustoeventhubdataconnection) AS kustoeventhubdataconnectionMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view kustoeventhubdataconnectionUserAccess_view AS \n","                            Select kustoeventhubdataconnectionmetadata_view.*, kustoeventhubdataconnectionMetadata.name AS ArtifactName, explode(kustoeventhubdataconnectionMetadata.users) as kustoeventhubdataconnectionUserAccess from kustoeventhubdataconnectionmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'kustoeventhubdataconnection' AS ArtifactType, ArtifactName, kustoeventhubdataconnectionUserAccess.artifactUserAccessRight AS AccessType,\n","                           kustoeventhubdataconnectionUserAccess.displayName AS UserName, kustoeventhubdataconnectionUserAccess.emailAddress AS UserEmail, kustoeventhubdataconnectionUserAccess.PrincipalType,\n","                           kustoeventhubdataconnectionUserAccess.userType from kustoeventhubdataconnectionUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"reflexproject\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('reflexproject column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"reflexproject\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('reflexproject column has value')    \n","\n","                    # explode reflexproject metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view reflexprojectMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(reflexproject) AS reflexprojectMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view reflexprojectUserAccess_view AS \n","                            Select reflexprojectmetadata_view.*, reflexprojectMetadata.name AS ArtifactName, explode(reflexprojectMetadata.users) as reflexprojectUserAccess from reflexprojectmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'reflexproject' AS ArtifactType, ArtifactName, reflexprojectUserAccess.artifactUserAccessRight AS AccessType,\n","                           reflexprojectUserAccess.displayName AS UserName, reflexprojectUserAccess.emailAddress AS UserEmail, reflexprojectUserAccess.PrincipalType,\n","                           reflexprojectUserAccess.userType from reflexprojectUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"reports\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('reports column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"reports\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('reports column has value')    \n","\n","                    # explode reports metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view reportsMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(reports) AS reportsMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view reportsUserAccess_view AS \n","                            Select reportsmetadata_view.*, reportsMetadata.name AS ArtifactName, explode(reportsMetadata.users) as reportsUserAccess from reportsmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'reports' AS ArtifactType, ArtifactName, reportsUserAccess.reportUserAccessRight AS AccessType,\n","                           reportsUserAccess.displayName AS UserName, reportsUserAccess.emailAddress AS UserEmail, reportsUserAccess.PrincipalType,\n","                           reportsUserAccess.userType from reportsUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        # if (\"users\" not in df_viewcolumns.columns):\n","        #     pass\n","        # else:\n","        #     print('users column present')\n","\n","        #     # explode users metadata\n","        \t\n","        #     result = spark.sql(\"\"\"\n","        #             create or replace temp view usersMetadata_view AS\n","        #             SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","        #             domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","        #             explode(users) AS usersMetadata FROM WorkspaceMetadata_view\n","        #             \"\"\")\n","\n","        #     # Create uniform format\n","        #     result = spark.sql(\"\"\"\n","        #                 create or replace temp view usersUserAccess_view AS \n","        #                 Select usersmetadata_view.*, usersMetadata.displayName AS ArtifactName, explode(usersMetadata.users) as usersUserAccess from usersmetadata_view\n","        #             \"\"\")\n","\n","        #     result = spark.sql(\"\"\"\n","        #             SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","        #                    WorkspaceState,'users' AS ArtifactType, ArtifactName, usersUserAccess.groupUserAccessRight AS AccessType,\n","        #                    usersUserAccess.displayName AS UserName, usersUserAccess.emailAddress AS UserEmail, usersUserAccess.PrincipalType,\n","        #                    usersUserAccess.userType from usersUserAccess_view          \n","        #             \"\"\")   \n","        #     #display(result)\n","\n","        #     # Insert metadata to final table - WorkspaceAccessMetadata\n","        #     result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n","\n","        if (\"warehouses\" not in df_viewcolumns.columns):\n","            pass\n","        else:\n","            print('warehouses column present')\n","\n","            # Check the value of the field\n","            for row in df_viewcolumns.collect():\n","                field_value = row[\"warehouses\"]\n","                #display(len(field_value))\n","                if len(field_value) == 0:\n","                   pass\n","                else:\n","                    print('warehouses column has value')    \n","\n","                    # explode warehouses metadata\n","        \t\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view warehousesMetadata_view AS\n","                            SELECT id AS WorkspaceId, name AS WorkspaceName, defaultDatasetStorageFormat AS WorkspaceDefaultDatasetStorageFormat, \n","                            domainId AS WorkspaceDomainId, isOnDedicatedCapacity AS isWorkspaceOnDedicatedCapacity, state AS WorkspaceState, \n","                            explode(warehouses) AS warehousesMetadata FROM WorkspaceMetadata_view\n","                        \"\"\")\n","\n","                    # Create uniform format\n","                    result = spark.sql(\"\"\"\n","                            create or replace temp view warehousesUserAccess_view AS \n","                            Select warehousesmetadata_view.*, warehousesMetadata.name AS ArtifactName, explode(warehousesMetadata.users) as warehousesUserAccess from warehousesmetadata_view\n","                        \"\"\")\n","\n","                    result = spark.sql(\"\"\"\n","                           SELECT WorkspaceId, WorkspaceName, WorkspaceDefaultDatasetStorageFormat, WorkspaceDomainId, isWorkspaceOnDedicatedCapacity, \n","                           WorkspaceState,'warehouses' AS ArtifactType, ArtifactName, warehousesUserAccess.datamartUserAccessRight AS AccessType,\n","                           warehousesUserAccess.displayName AS UserName, warehousesUserAccess.emailAddress AS UserEmail, warehousesUserAccess.PrincipalType,\n","                           warehousesUserAccess.userType from warehousesUserAccess_view          \n","                        \"\"\")   \n","                    #display(result)\n","\n","                    # Insert metadata to final table - WorkspaceAccessMetadata\n","                    result.write.format(\"delta\").mode(\"append\").save(myTablePath)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"805ff07c-7edf-4e93-b7fc-c75c6a3c70c4","statement_id":27,"state":"finished","livy_statement_state":"available","queued_time":"2023-12-22T16:17:56.3954088Z","session_start_time":null,"execution_start_time":"2023-12-22T16:17:59.6295932Z","execution_finish_time":"2023-12-22T16:20:10.6033155Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":0,"RUNNING":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"261ad045-c9ab-4cf7-8ec6-75ae221b6903"},"text/plain":"StatementMeta(, 805ff07c-7edf-4e93-b7fc-c75c6a3c70c4, 27, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Workspaceid: 4b63d48a-82f6-4f27-bb75-7a1b8b22df7e\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid 4b63d48a-82f6-4f27-bb75-7a1b8b22df7e is d37b8b38-9956-4261-9898-8a59380b9635'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid d37b8b38-9956-4261-9898-8a59380b9635 for Workspaceid 4b63d48a-82f6-4f27-bb75-7a1b8b22df7e is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: d37b8b38-9956-4261-9898-8a59380b9635 is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Valid workspace - do the processing\nnotebook column present\nnotebook column has value\nTable created\nLakehouse column present\nLakehouse column has value\nDataPipeline column present\nDataPipeline column has value\nEventstream column present\nEventstream column has value\nKQLDatabase column present\nKQLDatabase column has value\nKQLQueryset column present\nKQLQueryset column has value\nMLExperiment column present\nMLExperiment column has value\nMLModel column present\nMLModel column has value\nSQLAnalyticsEndpoint column present\nSQLAnalyticsEndpoint column has value\nSparkJobDefinition column present\nSparkJobDefinition column has value\ndashboards column present\ndashboards column has value\ndataflows column present\ndataflows column has value\ndatasets column present\ndatasets column has value\nenvironment column present\nenvironment column has value\nkustoeventhubdataconnection column present\nkustoeventhubdataconnection column has value\nreflexproject column present\nreflexproject column has value\nreports column present\nreports column has value\nwarehouses column present\nwarehouses column has value\nWorkspaceid: edf1fd48-b561-4540-82fc-37aed98c4963\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid edf1fd48-b561-4540-82fc-37aed98c4963 is 7ed441c0-9ea1-45c1-ad45-6aff22d6033c'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid 7ed441c0-9ea1-45c1-ad45-6aff22d6033c for Workspaceid edf1fd48-b561-4540-82fc-37aed98c4963 is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: 7ed441c0-9ea1-45c1-ad45-6aff22d6033c is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Valid workspace - do the processing\nnotebook column present\nnotebook column has value\nTable present, Data inserted\nLakehouse column present\nLakehouse column has value\nEventstream column present\nEventstream column has value\nKQLDatabase column present\nKQLDatabase column has value\nMLExperiment column present\nMLExperiment column has value\nMLModel column present\nMLModel column has value\nSQLAnalyticsEndpoint column present\nSQLAnalyticsEndpoint column has value\ndashboards column present\ndashboards column has value\ndataflows column present\ndataflows column has value\ndatasets column present\ndatasets column has value\nreflexproject column present\nreflexproject column has value\nreports column present\nreports column has value\nwarehouses column present\nwarehouses column has value\nWorkspaceid: e356b66e-9a99-49a9-886f-4a6055a26644\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid e356b66e-9a99-49a9-886f-4a6055a26644 is 7556f1d7-3bed-44ad-bf96-62e9ce3d9bea'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid 7556f1d7-3bed-44ad-bf96-62e9ce3d9bea for Workspaceid e356b66e-9a99-49a9-886f-4a6055a26644 is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: 7556f1d7-3bed-44ad-bf96-62e9ce3d9bea is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Valid workspace - do the processing\ndashboards column present\ndataflows column present\ndatasets column present\nreports column present\nWorkspaceid: 29e068aa-3094-4347-9afd-30c8167ba35e\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid 29e068aa-3094-4347-9afd-30c8167ba35e is 1414638d-0fb8-4c8d-9570-9508507bb7cd'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid 1414638d-0fb8-4c8d-9570-9508507bb7cd for Workspaceid 29e068aa-3094-4347-9afd-30c8167ba35e is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: 1414638d-0fb8-4c8d-9570-9508507bb7cd is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Workspaceid: bb6157b3-92fb-4995-a2ae-c511bd00b75b\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid bb6157b3-92fb-4995-a2ae-c511bd00b75b is b536c2b6-7cc9-442d-b34f-dd24e6753790'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid b536c2b6-7cc9-442d-b34f-dd24e6753790 for Workspaceid bb6157b3-92fb-4995-a2ae-c511bd00b75b is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: b536c2b6-7cc9-442d-b34f-dd24e6753790 is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Workspaceid: 6da79f32-7624-4815-a64c-325ea59b39d4\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid 6da79f32-7624-4815-a64c-325ea59b39d4 is fbf1e200-feb1-4c93-bcd4-46c66644c56b'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid fbf1e200-feb1-4c93-bcd4-46c66644c56b for Workspaceid 6da79f32-7624-4815-a64c-325ea59b39d4 is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: fbf1e200-feb1-4c93-bcd4-46c66644c56b is complete'"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Workspaceid: e26a57cd-fa71-4b01-8ea3-6190d4be4369\n"]},{"output_type":"display_data","data":{"text/plain":"'Scanid for Workspaceid e26a57cd-fa71-4b01-8ea3-6190d4be4369 is cb02f728-fa5b-4893-a029-f0a7bed4406a'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Status for Scanid cb02f728-fa5b-4893-a029-f0a7bed4406a for Workspaceid e26a57cd-fa71-4b01-8ea3-6190d4be4369 is Succeeded'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Scan for Scanid: cb02f728-fa5b-4893-a029-f0a7bed4406a is complete'"},"metadata":{}}],"execution_count":25,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"4f9f542c-11b8-442b-9e66-a0c85d348789"},{"cell_type":"markdown","source":["# Change this to code cell when you need to delete data from the Metadata table to start afresh\n","%%sql\n","DELETE from lakehouse01.WorkspaceAccessMetadata"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"36e6efc4-e412-467a-8b96-f0e2e640b245"},{"cell_type":"markdown","source":["%%sql\n","drop table lakehouse01.WorkspaceAccessMetadata"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"64cba1f5-454d-45c3-b035-dbaf81af2b53"}],"metadata":{"microsoft":{"host":{"synapse_widget":{"token":"5c943872-5844-4fe0-b4b8-af4a4905033d","state":{"7ac9c434-618c-43ac-a716-6d3169dcaa5d":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"2C6D6A3D-8332-4CB5-AA3B-415465744608","1":[],"2":[],"3":[],"4":[],"5":"Large","6":"3a48d8cf-b2f9-4e98-a42e-2a7ad318d87e","7":"e356b66e-9a99-49a9-886f-4a6055a26644","8":"true","9":"WS_SagarFabric01_Test","10":[],"11":"Active","12":"Workspace","13":[{"displayName":"Sagar Bathe","identifier":"sagarbathe@mngenvmcap129866.onmicrosoft.com","emailAddress":"sagarbathe@mngenvmcap129866.onmicrosoft.com","groupUserAccessRight":"Admin","graphId":"1dff36d4-e518-4823-8eae-e425daf7f54e","userType":"Member","principalType":"User"}],"index":1}],"schema":[{"key":"0","name":"capacityId","type":"string"},{"key":"1","name":"dashboards","type":"ArrayType(StringType,true)"},{"key":"2","name":"dataflows","type":"ArrayType(StringType,true)"},{"key":"3","name":"datamarts","type":"ArrayType(StringType,true)"},{"key":"4","name":"datasets","type":"ArrayType(StringType,true)"},{"key":"5","name":"defaultDatasetStorageFormat","type":"string"},{"key":"6","name":"domainId","type":"string"},{"key":"7","name":"id","type":"string"},{"key":"8","name":"isOnDedicatedCapacity","type":"boolean"},{"key":"9","name":"name","type":"string"},{"key":"10","name":"reports","type":"ArrayType(StringType,true)"},{"key":"11","name":"state","type":"string"},{"key":"12","name":"type","type":"string"},{"key":"13","name":"users","type":"ArrayType(StructType(StructField(displayName,StringType,true),StructField(emailAddress,StringType,true),StructField(graphId,StringType,true),StructField(groupUserAccessRight,StringType,true),StructField(identifier,StringType,true),StructField(principalType,StringType,true),StructField(userType,StringType,true)),true)"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["5"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"015eb7ba-ebd8-4027-9d44-1372617b7a12":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":[],"index":1}],"schema":[{"key":"0","name":"dashboards","type":"ArrayType(StringType,true)"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"2391c13c-002b-41b7-84ab-290b4e530e25":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"01 -  Ingest data into Trident lakehouse using Apache Spark","state":"Active","lastUpdatedDate":"2023-08-03T19:31:52.5318114","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"d2f65175-9fc3-43d7-bca9-7e4a31286384","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":1},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"02 - Explore and Visualize Data using Notebooks","state":"Active","lastUpdatedDate":"2023-08-03T19:33:07.0316888","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"e20fbdc1-47c9-48e6-8af2-28898e8e2e77","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":2},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"03 - Perform Data Cleansing and preparation using Apache Spark","state":"Active","lastUpdatedDate":"2023-07-24T17:23:11.2756771","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"6fc95877-bfcf-4926-b3ac-cc9d95b00491","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":3},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"04 - Train and track machine learning models","state":"Active","lastUpdatedDate":"2023-08-03T19:36:54.0620569","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"53a05a1d-06b7-4a7d-b4dc-faab8380f65a","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":4},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"05 - Perform batch scoring and save predictions to lakehouse","state":"Active","lastUpdatedDate":"2023-07-24T18:12:47.7225501","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"ca9821c8-26b0-4689-83f2-742ab9cfcf5e","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":5},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Data Science Notebook 1","state":"Active","lastUpdatedDate":"2023-12-07T16:37:42.1177887","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"c0956c99-f022-413f-b3fa-3a776e507eba","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":6},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Fabric Delta Streaming Notebook 1","state":"Active","lastUpdatedDate":"2023-07-07T16:13:40.824197","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"d93dabe0-102e-47d9-acd8-5d3fba53c7a4","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":7},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Fabric Spark Notebook 1","state":"Active","lastUpdatedDate":"2023-09-26T18:28:15.386443","endorsementDetails":{"certifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","endorsement":"Promoted"},"relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"a2eef56c-c697-4342-a0ba-db2c7433647c","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":8},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"lakehouse01 analysis","state":"Active","lastUpdatedDate":"2023-11-09T17:21:26.2324755","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"aa675b42-2db2-4e0a-8c9b-eb6d75504c08","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":9},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Load Sales Notebook 1","state":"Active","lastUpdatedDate":"2023-12-08T16:23:26.800467","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"6c607c7a-8702-45c3-8cdc-3a47a49485ce","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":10},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"powerbi api test","state":"Active","lastUpdatedDate":"2023-12-15T18:15:01.8564456","createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"1a1f0371-470c-438b-a34b-f23837778938","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":11},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Sample notebook","state":"Active","lastUpdatedDate":"2023-09-25T21:32:16.4434605","createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"4bd42c34-a55a-4e71-8da1-5c06548478ca","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":12},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Sample_notebook_219","state":"Active","lastUpdatedDate":"2023-08-14T15:40:22.2818857","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"7dc24635-3fe6-4639-bf2e-78a1dee1e215","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"2ce87844-0dde-4a38-840d-16c6fb895898","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"Sample notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":13},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Scanner API demo","state":"Active","lastUpdatedDate":"2023-12-21T15:05:16.8270638","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"8ef81df7-40d3-45f6-a267-ea6f86268874","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":14},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Scanner API demo 20231218","state":"Active","lastUpdatedDate":"2023-12-18T21:07:22.6025465","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"2e18dac6-4cfc-4651-8f88-4cf04f960def","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":15},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Scanner API demo Copy","state":"Active","lastUpdatedDate":"2023-12-15T16:53:02.9705","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"4bda578f-3069-4fa9-9f0f-062d29ba45f8","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":16},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"SemanticKernel-demo","state":"Active","lastUpdatedDate":"2023-08-21T15:57:31.3282263","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"f8b2a7ab-a84a-4393-ae23-76f929df61e3","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":17}],"schema":[{"key":"0","name":"NoteBookMetadata","type":"StructType(StructField(createdBy,StringType,true),StructField(createdById,StringType,true),StructField(description,StringType,true),StructField(endorsementDetails,StructType(StructField(certifiedBy,StringType,true),StructField(endorsement,StringType,true)),true),StructField(id,StringType,true),StructField(lastUpdatedDate,StringType,true),StructField(modifiedBy,StringType,true),StructField(modifiedById,StringType,true),StructField(name,StringType,true),StructField(relations,ArrayType(StructType(StructField(dependentOnArtifactId,StringType,true),StructField(relationType,StringType,true),StructField(settingsList,StringType,true),StructField(usage,StringType,true),StructField(workspaceId,StringType,true)),true),true),StructField(state,StringType,true))"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"01f95bab-32c5-4ec7-9728-335cdb2a8d24":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"7f868d34-45c6-4291-9852-331c7d04ce63","1":"2023-12-21T19:59:55.4810521Z","2":"NotStarted","index":1}],"schema":[{"key":"0","name":"id","type":"string"},{"key":"1","name":"createdDateTime","type":"string"},{"key":"2","name":"status","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"c0fcf5a3-953a-4eb0-aaa2-97c3874254ff":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":[{"name":"WS_SagarFabric01_Test","dashboards":[],"state":"Active","users":[{"displayName":"Sagar Bathe","identifier":"sagarbathe@mngenvmcap129866.onmicrosoft.com","emailAddress":"sagarbathe@mngenvmcap129866.onmicrosoft.com","groupUserAccessRight":"Admin","graphId":"1dff36d4-e518-4823-8eae-e425daf7f54e","userType":"Member","principalType":"User"}],"dataflows":[],"datasets":[],"datamarts":[],"id":"e356b66e-9a99-49a9-886f-4a6055a26644","reports":[],"domainId":"3a48d8cf-b2f9-4e98-a42e-2a7ad318d87e","type":"Workspace","defaultDatasetStorageFormat":"Large","capacityId":"2C6D6A3D-8332-4CB5-AA3B-415465744608","isOnDedicatedCapacity":true}],"index":1}],"schema":[{"key":"0","name":"workspaces","type":"ArrayType(StructType(StructField(capacityId,StringType,true),StructField(dashboards,ArrayType(StringType,true),true),StructField(dataflows,ArrayType(StringType,true),true),StructField(datamarts,ArrayType(StringType,true),true),StructField(datasets,ArrayType(StringType,true),true),StructField(defaultDatasetStorageFormat,StringType,true),StructField(domainId,StringType,true),StructField(id,StringType,true),StructField(isOnDedicatedCapacity,BooleanType,true),StructField(name,StringType,true),StructField(reports,ArrayType(StringType,true),true),StructField(state,StringType,true),StructField(type,StringType,true),StructField(users,ArrayType(StructType(StructField(displayName,StringType,true),StructField(emailAddress,StringType,true),StructField(graphId,StringType,true),StructField(groupUserAccessRight,StringType,true),StructField(identifier,StringType,true),StructField(principalType,StringType,true),StructField(userType,StringType,true)),true),true)),true)"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}}},"language":"python"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{"7ac9c434-618c-43ac-a716-6d3169dcaa5d":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"2C6D6A3D-8332-4CB5-AA3B-415465744608","1":[],"2":[],"3":[],"4":[],"5":"Large","6":"3a48d8cf-b2f9-4e98-a42e-2a7ad318d87e","7":"e356b66e-9a99-49a9-886f-4a6055a26644","8":"true","9":"WS_SagarFabric01_Test","10":[],"11":"Active","12":"Workspace","13":[{"displayName":"Sagar Bathe","identifier":"sagarbathe@mngenvmcap129866.onmicrosoft.com","emailAddress":"sagarbathe@mngenvmcap129866.onmicrosoft.com","groupUserAccessRight":"Admin","graphId":"1dff36d4-e518-4823-8eae-e425daf7f54e","userType":"Member","principalType":"User"}],"index":1}],"schema":[{"key":"0","name":"capacityId","type":"string"},{"key":"1","name":"dashboards","type":"ArrayType(StringType,true)"},{"key":"2","name":"dataflows","type":"ArrayType(StringType,true)"},{"key":"3","name":"datamarts","type":"ArrayType(StringType,true)"},{"key":"4","name":"datasets","type":"ArrayType(StringType,true)"},{"key":"5","name":"defaultDatasetStorageFormat","type":"string"},{"key":"6","name":"domainId","type":"string"},{"key":"7","name":"id","type":"string"},{"key":"8","name":"isOnDedicatedCapacity","type":"boolean"},{"key":"9","name":"name","type":"string"},{"key":"10","name":"reports","type":"ArrayType(StringType,true)"},{"key":"11","name":"state","type":"string"},{"key":"12","name":"type","type":"string"},{"key":"13","name":"users","type":"ArrayType(StructType(StructField(displayName,StringType,true),StructField(emailAddress,StringType,true),StructField(graphId,StringType,true),StructField(groupUserAccessRight,StringType,true),StructField(identifier,StringType,true),StructField(principalType,StringType,true),StructField(userType,StringType,true)),true)"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["5"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"015eb7ba-ebd8-4027-9d44-1372617b7a12":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":[],"index":1}],"schema":[{"key":"0","name":"dashboards","type":"ArrayType(StringType,true)"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"2391c13c-002b-41b7-84ab-290b4e530e25":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"01 -  Ingest data into Trident lakehouse using Apache Spark","state":"Active","lastUpdatedDate":"2023-08-03T19:31:52.5318114","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"d2f65175-9fc3-43d7-bca9-7e4a31286384","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":1},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"02 - Explore and Visualize Data using Notebooks","state":"Active","lastUpdatedDate":"2023-08-03T19:33:07.0316888","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"e20fbdc1-47c9-48e6-8af2-28898e8e2e77","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":2},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"03 - Perform Data Cleansing and preparation using Apache Spark","state":"Active","lastUpdatedDate":"2023-07-24T17:23:11.2756771","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"6fc95877-bfcf-4926-b3ac-cc9d95b00491","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":3},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"04 - Train and track machine learning models","state":"Active","lastUpdatedDate":"2023-08-03T19:36:54.0620569","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"53a05a1d-06b7-4a7d-b4dc-faab8380f65a","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":4},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"05 - Perform batch scoring and save predictions to lakehouse","state":"Active","lastUpdatedDate":"2023-07-24T18:12:47.7225501","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"ca9821c8-26b0-4689-83f2-742ab9cfcf5e","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":5},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Data Science Notebook 1","state":"Active","lastUpdatedDate":"2023-12-07T16:37:42.1177887","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"c0956c99-f022-413f-b3fa-3a776e507eba","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":6},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Fabric Delta Streaming Notebook 1","state":"Active","lastUpdatedDate":"2023-07-07T16:13:40.824197","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"d93dabe0-102e-47d9-acd8-5d3fba53c7a4","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":7},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Fabric Spark Notebook 1","state":"Active","lastUpdatedDate":"2023-09-26T18:28:15.386443","endorsementDetails":{"certifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","endorsement":"Promoted"},"relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"a2eef56c-c697-4342-a0ba-db2c7433647c","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":8},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"lakehouse01 analysis","state":"Active","lastUpdatedDate":"2023-11-09T17:21:26.2324755","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"aa675b42-2db2-4e0a-8c9b-eb6d75504c08","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":9},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Load Sales Notebook 1","state":"Active","lastUpdatedDate":"2023-12-08T16:23:26.800467","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"6c607c7a-8702-45c3-8cdc-3a47a49485ce","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":10},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"powerbi api test","state":"Active","lastUpdatedDate":"2023-12-15T18:15:01.8564456","createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"1a1f0371-470c-438b-a34b-f23837778938","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":11},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Sample notebook","state":"Active","lastUpdatedDate":"2023-09-25T21:32:16.4434605","createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"4bd42c34-a55a-4e71-8da1-5c06548478ca","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":12},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Sample_notebook_219","state":"Active","lastUpdatedDate":"2023-08-14T15:40:22.2818857","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"7dc24635-3fe6-4639-bf2e-78a1dee1e215","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"2ce87844-0dde-4a38-840d-16c6fb895898","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"Sample notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":13},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Scanner API demo","state":"Active","lastUpdatedDate":"2023-12-21T15:05:16.8270638","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"8ef81df7-40d3-45f6-a267-ea6f86268874","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":14},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Scanner API demo 20231218","state":"Active","lastUpdatedDate":"2023-12-18T21:07:22.6025465","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"2e18dac6-4cfc-4651-8f88-4cf04f960def","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":15},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"Scanner API demo Copy","state":"Active","lastUpdatedDate":"2023-12-15T16:53:02.9705","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"4bda578f-3069-4fa9-9f0f-062d29ba45f8","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":16},{"0":{"modifiedBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com","name":"SemanticKernel-demo","state":"Active","lastUpdatedDate":"2023-08-21T15:57:31.3282263","relations":[{"workspaceId":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e","dependentOnArtifactId":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","usage":"Datasource","relationType":"Datasource","settingsList":"None"}],"createdById":"1dff36d4-e518-4823-8eae-e425daf7f54e","id":"f8b2a7ab-a84a-4393-ae23-76f929df61e3","modifiedById":"1dff36d4-e518-4823-8eae-e425daf7f54e","description":"New notebook","createdBy":"sagarbathe@mngenvmcap129866.onmicrosoft.com"},"index":17}],"schema":[{"key":"0","name":"NoteBookMetadata","type":"StructType(StructField(createdBy,StringType,true),StructField(createdById,StringType,true),StructField(description,StringType,true),StructField(endorsementDetails,StructType(StructField(certifiedBy,StringType,true),StructField(endorsement,StringType,true)),true),StructField(id,StringType,true),StructField(lastUpdatedDate,StringType,true),StructField(modifiedBy,StringType,true),StructField(modifiedById,StringType,true),StructField(name,StringType,true),StructField(relations,ArrayType(StructType(StructField(dependentOnArtifactId,StringType,true),StructField(relationType,StringType,true),StructField(settingsList,StringType,true),StructField(usage,StringType,true),StructField(workspaceId,StringType,true)),true),true),StructField(state,StringType,true))"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"01f95bab-32c5-4ec7-9728-335cdb2a8d24":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"7f868d34-45c6-4291-9852-331c7d04ce63","1":"2023-12-21T19:59:55.4810521Z","2":"NotStarted","index":1}],"schema":[{"key":"0","name":"id","type":"string"},{"key":"1","name":"createdDateTime","type":"string"},{"key":"2","name":"status","type":"string"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}},"c0fcf5a3-953a-4eb0-aaa2-97c3874254ff":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":[{"name":"WS_SagarFabric01_Test","dashboards":[],"state":"Active","users":[{"displayName":"Sagar Bathe","identifier":"sagarbathe@mngenvmcap129866.onmicrosoft.com","emailAddress":"sagarbathe@mngenvmcap129866.onmicrosoft.com","groupUserAccessRight":"Admin","graphId":"1dff36d4-e518-4823-8eae-e425daf7f54e","userType":"Member","principalType":"User"}],"dataflows":[],"datasets":[],"datamarts":[],"id":"e356b66e-9a99-49a9-886f-4a6055a26644","reports":[],"domainId":"3a48d8cf-b2f9-4e98-a42e-2a7ad318d87e","type":"Workspace","defaultDatasetStorageFormat":"Large","capacityId":"2C6D6A3D-8332-4CB5-AA3B-415465744608","isOnDedicatedCapacity":true}],"index":1}],"schema":[{"key":"0","name":"workspaces","type":"ArrayType(StructType(StructField(capacityId,StringType,true),StructField(dashboards,ArrayType(StringType,true),true),StructField(dataflows,ArrayType(StringType,true),true),StructField(datamarts,ArrayType(StringType,true),true),StructField(datasets,ArrayType(StringType,true),true),StructField(defaultDatasetStorageFormat,StringType,true),StructField(domainId,StringType,true),StructField(id,StringType,true),StructField(isOnDedicatedCapacity,BooleanType,true),StructField(name,StringType,true),StructField(reports,ArrayType(StringType,true),true),StructField(state,StringType,true),StructField(type,StringType,true),StructField(users,ArrayType(StructType(StructField(displayName,StringType,true),StructField(emailAddress,StringType,true),StructField(graphId,StringType,true),StructField(groupUserAccessRight,StringType,true),StructField(identifier,StringType,true),StructField(principalType,StringType,true),StructField(userType,StringType,true)),true),true)),true)"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f"}],"default_lakehouse":"f1403863-70cc-49c3-94f5-c4a4b0c7ac9f","default_lakehouse_name":"lakehouse01","default_lakehouse_workspace_id":"4b63d48a-82f6-4f27-bb75-7a1b8b22df7e"}}},"nbformat":4,"nbformat_minor":5}